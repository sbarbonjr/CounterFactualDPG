{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyzfiPJKA-jM",
    "outputId": "b5c08493-799d-45b1-f13d-34c7743101fd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive\n",
    "import sys\n",
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "PATH = '/content/'\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uEFw17RoBb87",
    "outputId": "3fd423d3-dacc-451d-e23a-6adb92d2fce5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor  # keep if you'll use it later\n",
    "\n",
    "df = pd.read_csv(PATH + \"comparison_numeric_small.csv\")\n",
    "\n",
    "# Quick look\n",
    "print(\"Original shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# 1) Remove a specific dataset by name\n",
    "#    (here I assume the column is literally called 'Dataset')\n",
    "bad_datasets = [\"breast_cancer_wisconsin\"]\n",
    "df = df[~df[\"Dataset\"].isin(bad_datasets)]\n",
    "\n",
    "print(\"\\nAfter removing specific datasets:\", bad_datasets)\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "# 2) Identify metric columns (the ones with _DPG / _DiCE suffixes)\n",
    "suffix_dpg = \"_DPG\"\n",
    "suffix_baseline = \"_DiCE\"\n",
    "\n",
    "metric_cols = [c for c in df.columns if c.endswith(suffix_dpg) or c.endswith(suffix_baseline)]\n",
    "print(\"\\nMetric columns used to check NaNs:\")\n",
    "print(metric_cols)\n",
    "\n",
    "# 3) Check NaN counts per dataset (optional, just for inspection)\n",
    "nan_counts = df[metric_cols].isna().sum(axis=1)\n",
    "print(\"\\nNaN counts per dataset row (before dropping):\")\n",
    "display(pd.DataFrame({\n",
    "    \"Dataset\": df[\"Dataset\"],\n",
    "    \"nan_count_metrics\": nan_counts\n",
    "}))\n",
    "\n",
    "# 4) Drop rows (datasets) that have ANY NaN in these metric columns\n",
    "df_clean = df.dropna(subset=metric_cols)\n",
    "\n",
    "print(\"\\nAfter dropping rows with NaNs in metric columns:\")\n",
    "print(\"Shape:\", df_clean.shape)\n",
    "\n",
    "# Show remaining datasets\n",
    "print(\"\\nRemaining datasets:\")\n",
    "print(df_clean[\"Dataset\"].tolist())\n",
    "\n",
    "# If you want to continue with heatmaps or stats, use df_clean from now on\n",
    "\n",
    "bad_datasets = [\"abalone_19\", \"wheat-seeds\", \"heart_disease_uci\"]  # optionally: add \"heart_disease_uci\"\n",
    "df_clean = df_clean[~df_clean[\"Dataset\"].isin(bad_datasets)]\n",
    "\n",
    "df = df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nr37ZvorG72P",
    "outputId": "8d8e78c4-2d11-4947-d016-906efcdd39f1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 0) Use dataset name as index (adapt this to your actual column name) ---\n",
    "# Try some common possibilities; if none exist, we keep the numeric index\n",
    "possible_name_cols = [\"dataset\", \"Dataset\", \"data\", \"Data\", \"name\", \"Name\"]\n",
    "name_col = None\n",
    "for c in possible_name_cols:\n",
    "    if c in df.columns:\n",
    "        name_col = c\n",
    "        break\n",
    "\n",
    "if name_col is not None:\n",
    "    df = df.set_index(name_col)\n",
    "    print(f\"Using '{name_col}' as dataset index.\")\n",
    "else:\n",
    "    print(\"No dataset-name column found; using numeric index as dataset ID.\")\n",
    "\n",
    "print(\"Index (datasets):\", df.index.tolist())\n",
    "\n",
    "# --- 1) Detect metrics (base names) present for both DPG and DiCE ---\n",
    "suffix_dpg = \"_DPG\"\n",
    "suffix_baseline = \"_DiCE\"  # use the exact suffix in your CSV\n",
    "\n",
    "cols_dpg = [c for c in df.columns if c.endswith(suffix_dpg)]\n",
    "cols_base = [c for c in df.columns if c.endswith(suffix_baseline)]\n",
    "\n",
    "metrics = sorted(\n",
    "    set(c.replace(suffix_dpg, \"\") for c in cols_dpg)\n",
    "    & set(c.replace(suffix_baseline, \"\") for c in cols_base)\n",
    ")\n",
    "\n",
    "print(\"Metrics used in heatmap:\", metrics)\n",
    "\n",
    "# --- 2) Build matrix of differences (DPG - DiCE) ---\n",
    "diff_mat = pd.DataFrame(index=df.index)\n",
    "\n",
    "for metric in metrics:\n",
    "    col_dpg = metric + suffix_dpg\n",
    "    col_base = metric + suffix_baseline\n",
    "    diff_mat[metric] = df[col_dpg] - df[col_base]\n",
    "\n",
    "display(diff_mat)\n",
    "\n",
    "# --- 3) Column-wise normalization to [-1, 1] by max absolute value per metric ---\n",
    "diff_mat_norm = diff_mat.copy()\n",
    "\n",
    "for col in diff_mat_norm.columns:\n",
    "    max_abs = np.nanmax(np.abs(diff_mat_norm[col].values))\n",
    "    if max_abs > 0:\n",
    "        diff_mat_norm[col] = diff_mat_norm[col] / max_abs\n",
    "    else:\n",
    "        diff_mat_norm[col] = 0.0  # all zeros/NaNs\n",
    "\n",
    "display(diff_mat_norm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "sYsRHW4ss4fW",
    "outputId": "a232a698-e4b2-4dc9-fa9a-aa913a948f0f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# diff_mat and diff_mat_norm must already be defined:\n",
    "#   diff_mat:  original differences (DPG-CF - DiCE) per dataset x metric\n",
    "#   diff_mat_norm: same shape, column-wise normalised to [-1, 1]\n",
    "\n",
    "# --- 1) Internal metric -> pretty label ---\n",
    "metric_name_map = {\n",
    "    \"plausibility_nbr_cf\":   \"Implausibility\",\n",
    "    \"count_diversity_all\":   \"Diversity\",\n",
    "    \"avg_nbr_changes\":       \"Sparsity\",\n",
    "    \"accuracy_knn_sklearn\":  \"Discriminative Power\",\n",
    "    \"runtime\":               \"Runtime\",\n",
    "    \"distance_mh\":           \"Distance\",\n",
    "    \"perc_valid_cf_all\":     \"Validity\",\n",
    "    \"perc_actionable_cf_all\":\"Actionability\",\n",
    "}\n",
    "\n",
    "# --- 2) Internal metric -> goal arrow (↑ or ↓) ---\n",
    "metric_goal_map = {\n",
    "    \"plausibility_nbr_cf\":   \"↓\",  # higher plausibility is better\n",
    "    \"count_diversity_all\":   \"↑\",\n",
    "    \"avg_nbr_changes\":       \"↓\",\n",
    "    \"accuracy_knn_sklearn\":  \"↑\",\n",
    "    \"runtime\":               \"↓\",\n",
    "    \"distance_mh\":           \"↓\",\n",
    "    \"perc_valid_cf_all\":     \"↑\",\n",
    "    \"perc_actionable_cf_all\":\"↑\",\n",
    "}\n",
    "\n",
    "# --- 3) Keep only metrics present in diff_mat and sort alphabetically by pretty label ---\n",
    "metrics_present = [m for m in metric_name_map.keys() if m in diff_mat.columns]\n",
    "\n",
    "# sort by pretty label (Actionability, Distance, Diversity, ... )\n",
    "base_metrics_sorted = sorted(\n",
    "    metrics_present,\n",
    "    key=lambda m: metric_name_map[m]\n",
    ")\n",
    "\n",
    "# --- 4) Reorder matrices ---\n",
    "diff_mat_plot = diff_mat[base_metrics_sorted]\n",
    "diff_mat_norm_plot = diff_mat_norm[base_metrics_sorted]\n",
    "\n",
    "# --- 5) Build x-axis labels with arrows, e.g. \"Distance ↓\" ---\n",
    "x_labels = [\n",
    "    f\"{metric_name_map[m]} {metric_goal_map[m]}\"\n",
    "    for m in base_metrics_sorted\n",
    "]\n",
    "\n",
    "# --- 6) Plot heatmap ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.heatmap(\n",
    "    diff_mat_norm_plot,\n",
    "    annot=diff_mat_plot.round(2),  # show original DPG-CF - DiCE diffs\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1, vmax=1,\n",
    "    center=0,\n",
    "    linewidths=0.5,\n",
    "    linecolor=\"gray\",\n",
    "    cbar_kws={\"label\": \"Column-wise scaled difference\"}\n",
    ")\n",
    "\n",
    "ax.set_title(\"\", fontsize=14)\n",
    "ax.set_xlabel(\"Metric (goal)\", fontsize=12)\n",
    "ax.set_ylabel(\"Dataset\", fontsize=12)\n",
    "\n",
    "ax.set_xticklabels(x_labels, rotation=45, ha=\"right\", fontsize=10)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- 7) Save as PDF for LaTeX ---\n",
    "plt.savefig(PATH+\"heatmap_dpgcf_dice_metrics.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "30XYVc5LCkX8",
    "outputId": "3f1b2686-cf0c-4b93-b1b0-e05a1f26542c"
   },
   "outputs": [],
   "source": [
    "print(\"Columns:\", df.columns.tolist())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvOareD8DRtp",
    "outputId": "4756b8a2-4806-42ee-be06-c9bb99cb9500"
   },
   "outputs": [],
   "source": [
    "!pip install scipy pandas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "osHdzkqaCp39",
    "outputId": "521f83af-6708-4ff7-f7a5-e7ea4e530543"
   },
   "outputs": [],
   "source": [
    "# === 2. Detect metric pairs with suffixes ===\n",
    "suffix_dpg = \"_DPG\"\n",
    "suffix_baseline = \"_DiCE\"   # NOTE: exact capitalization from your CSV\n",
    "\n",
    "cols_dpg = [c for c in df.columns if c.endswith(suffix_dpg)]\n",
    "cols_base = [c for c in df.columns if c.endswith(suffix_baseline)]\n",
    "\n",
    "# Base metric names (before suffix)\n",
    "metrics = sorted(\n",
    "    set(c.replace(suffix_dpg, \"\") for c in cols_dpg)\n",
    "    & set(c.replace(suffix_baseline, \"\") for c in cols_base)\n",
    ")\n",
    "\n",
    "print(\"Detected metrics:\", metrics)\n",
    "\n",
    "# === 3. Holm–Bonferroni correction ===\n",
    "def holm_bonferroni(p_vals):\n",
    "    \"\"\"\n",
    "    p_vals: dict {metric_name: raw_p}\n",
    "    returns: dict {metric_name: corrected_p}\n",
    "    \"\"\"\n",
    "    items = sorted(p_vals.items(), key=lambda x: x[1])  # sort by p ascending\n",
    "    m = len(items)\n",
    "    corrected_internal = {}\n",
    "    prev = 0.0\n",
    "    for i, (name, p) in enumerate(items, start=1):\n",
    "        adj = min(1.0, (m - i + 1) * p)   # Holm step-down\n",
    "        # ensure monotonic increasing across sorted p's\n",
    "        adj = max(adj, prev)\n",
    "        prev = adj\n",
    "        corrected_internal[name] = adj\n",
    "\n",
    "    # restore original order\n",
    "    return {name: corrected_internal[name] for name in p_vals.keys()}\n",
    "\n",
    "# === 4. Run Wilcoxon per metric ===\n",
    "results = []\n",
    "raw_pvals = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    col_dpg = metric + suffix_dpg\n",
    "    col_base = metric + suffix_baseline\n",
    "\n",
    "    x = df[col_dpg]\n",
    "    y = df[col_base]\n",
    "\n",
    "    # Drop NaN pairs (e.g., when DiCE fails on some dataset)\n",
    "    mask = ~(x.isna() | y.isna())\n",
    "    x_clean = x[mask].values\n",
    "    y_clean = y[mask].values\n",
    "\n",
    "    if len(x_clean) < 1:\n",
    "        print(f\"Skipping metric {metric}: no valid pairs (all NaN).\")\n",
    "        continue\n",
    "\n",
    "    # Wilcoxon signed-rank on paired differences (DPG - baseline)\n",
    "    # zero_method='wilcox' ignores ties/zeros in the differences\n",
    "    stat, p = wilcoxon(\n",
    "        x_clean, y_clean,\n",
    "        zero_method='wilcox',\n",
    "        alternative='two-sided'\n",
    "    )\n",
    "\n",
    "    diff = x_clean - y_clean\n",
    "    median_diff = np.median(diff)\n",
    "\n",
    "    # Simple sign-based effect size: (n_pos - n_neg) / (n_pos + n_neg)\n",
    "    non_zero = diff[diff != 0]\n",
    "    n_pos = np.sum(non_zero > 0)\n",
    "    n_neg = np.sum(non_zero < 0)\n",
    "    if (n_pos + n_neg) > 0:\n",
    "        effect_sign = (n_pos - n_neg) / (n_pos + n_neg)\n",
    "    else:\n",
    "        effect_sign = 0.0  # all differences zero\n",
    "\n",
    "    raw_pvals[metric] = p\n",
    "\n",
    "    results.append({\n",
    "        \"metric\": metric,\n",
    "        \"n_datasets_used\": len(x_clean),\n",
    "        \"median_diff(DPG - DiCE)\": median_diff,\n",
    "        \"wilcoxon_stat\": stat,\n",
    "        \"p_value_raw\": p,\n",
    "        \"effect_size_sign\": effect_sign\n",
    "    })\n",
    "\n",
    "# If no metrics survived, stop gracefully\n",
    "if not results:\n",
    "    print(\"No metrics detected or no valid pairs to test.\")\n",
    "else:\n",
    "    # === 5. Apply Holm–Bonferroni correction across metrics ===\n",
    "    corrected = holm_bonferroni(raw_pvals)\n",
    "    for r in results:\n",
    "        r[\"p_value_holm\"] = corrected[r[\"metric\"]]\n",
    "\n",
    "    # === 6. Show results as a table ===\n",
    "    res_df = pd.DataFrame(results)\n",
    "    res_df = res_df.sort_values(\"p_value_raw\")\n",
    "\n",
    "    # Round for readability\n",
    "    for col in [\"median_diff(DPG - DiCE)\", \"p_value_raw\", \"p_value_holm\", \"effect_size_sign\"]:\n",
    "        res_df[col] = res_df[col].round(4)\n",
    "\n",
    "    display(res_df)\n",
    "\n",
    "    # Optional: get LaTeX for the paper\n",
    "    print(res_df.to_latex(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generate LaTeX table for paper ===\n",
    "\n",
    "# Metric name mapping (internal -> pretty)\n",
    "metric_name_map = {\n",
    "    \"plausibility_nbr_cf\":   \"Implausibility\",\n",
    "    \"count_diversity_all\":   \"Diversity\",\n",
    "    \"avg_nbr_changes\":       \"Sparsity\",\n",
    "    \"accuracy_knn_sklearn\":  \"Discriminative Power\",\n",
    "    \"runtime\":               \"Runtime\",\n",
    "    \"distance_mh\":           \"Distance\",\n",
    "    \"perc_valid_cf_all\":     \"Validity\",\n",
    "    \"perc_actionable_cf_all\":\"Actionability\",\n",
    "}\n",
    "\n",
    "# Metric goal mapping (internal -> LaTeX arrow command)\n",
    "metric_goal_map = {\n",
    "    \"plausibility_nbr_cf\":   (\"\\\\downarrow\", \"lower\"),\n",
    "    \"count_diversity_all\":   (\"\\\\uparrow\", \"higher\"),\n",
    "    \"avg_nbr_changes\":       (\"\\\\downarrow\", \"lower\"),\n",
    "    \"accuracy_knn_sklearn\":  (\"\\\\uparrow\", \"higher\"),\n",
    "    \"runtime\":               (\"\\\\downarrow\", \"lower\"),\n",
    "    \"distance_mh\":           (\"\\\\downarrow\", \"lower\"),\n",
    "    \"perc_valid_cf_all\":     (\"\\\\uparrow\", \"higher\"),\n",
    "    \"perc_actionable_cf_all\":(\"\\\\uparrow\", \"higher\"),\n",
    "}\n",
    "\n",
    "# Sort by pretty name alphabetically\n",
    "res_df_sorted = res_df.copy()\n",
    "res_df_sorted[\"pretty_name\"] = res_df_sorted[\"metric\"].map(metric_name_map)\n",
    "res_df_sorted = res_df_sorted.sort_values(\"pretty_name\")\n",
    "\n",
    "# Get the number of datasets used (should be same for all metrics)\n",
    "n_datasets = int(res_df_sorted.iloc[0][\"n_datasets_used\"])\n",
    "\n",
    "# Build LaTeX table rows\n",
    "latex_rows = []\n",
    "\n",
    "for _, row in res_df_sorted.iterrows():\n",
    "    metric = row[\"metric\"]\n",
    "    pretty_name = metric_name_map.get(metric, metric)\n",
    "    goal_symbol, goal_direction = metric_goal_map.get(metric, (\"\", \"\"))\n",
    "    \n",
    "    median_diff = row[\"median_diff(DPG - DiCE)\"]\n",
    "    p_value = row[\"p_value_raw\"]\n",
    "    \n",
    "    # Determine which method is better based on goal and median difference\n",
    "    is_significant = p_value < 0.05\n",
    "    \n",
    "    if abs(median_diff) < 1e-6:  # essentially zero\n",
    "        best_method = \"Both\"\n",
    "    elif goal_direction == \"higher\":\n",
    "        # Higher is better, so if median_diff > 0, DPG-CF is better\n",
    "        if median_diff > 0:\n",
    "            best_method = \"DPG-CF\"\n",
    "        else:\n",
    "            best_method = \"DiCE\"\n",
    "    else:  # goal_direction == \"lower\"\n",
    "        # Lower is better, so if median_diff < 0, DPG-CF is better\n",
    "        if median_diff < 0:\n",
    "            best_method = \"DPG-CF\"\n",
    "        else:\n",
    "            best_method = \"DiCE\"\n",
    "    \n",
    "    # Bold if significant and not \"Both\"\n",
    "    if is_significant and best_method != \"Both\":\n",
    "        best_method = f\"\\\\textbf{{{best_method}}}\"\n",
    "    \n",
    "    # Format the row exactly as in the example (without n column)\n",
    "    latex_row = f\"    \\\\texttt{{{pretty_name}}}  & ${goal_symbol}$ & {median_diff:6.2f}  & {p_value:.2f} & {best_method}            \\\\\\\\\"\n",
    "    latex_rows.append(latex_row)\n",
    "\n",
    "# Assemble full LaTeX table\n",
    "latex_table = f\"\"\"\\\\begin{{table}}[b!]\n",
    "  \\\\centering\n",
    "  \\\\caption{{Paired Wilcoxon signed-rank comparison between DPG-CF and DiCE across\n",
    "  {n_datasets} datasets. $\\\\Delta = \\\\text{{DPG-CF}} - \\\\text{{DiCE}}$. The ``Goal'' column indicates\n",
    "  whether higher ($\\\\uparrow$) or lower ($\\\\downarrow$) values are desirable. The\n",
    "  ``Best'' column reports the method with better median performance according to\n",
    "  this goal; method names in bold denote statistically significant differences\n",
    "  (Wilcoxon test, $p < 0.05$). All metrics computed using $n = {n_datasets}$ datasets.}}\n",
    "  \\\\label{{tab:wilcoxon_results}}\n",
    "  \\\\begin{{tabular}}{{lcccl}}\n",
    "    \\\\toprule\n",
    "    Metric & Goal & median $\\\\Delta$ & $p$-value & Best \\\\\\\\\n",
    "    \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "for row in latex_rows:\n",
    "    latex_table += row + \"\\n\"\n",
    "\n",
    "latex_table += r\"\"\"    \\bottomrule\n",
    "  \\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "z8FvSAueDqYs",
    "outputId": "c1e03f76-0d19-4436-ed47-d1f9314f9114"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build a long-form dataframe of differences per dataset & metric\n",
    "rows = []\n",
    "for metric in metrics:\n",
    "    col_dpg = metric + suffix_dpg\n",
    "    col_base = metric + suffix_baseline\n",
    "    if col_dpg not in df.columns or col_base not in df.columns:\n",
    "        continue\n",
    "\n",
    "    diffs = df[col_dpg] - df[col_base]\n",
    "    for dataset_name, dval in zip(df.index, diffs):\n",
    "        rows.append({\"metric\": metric, \"dataset\": dataset_name, \"diff\": dval})\n",
    "\n",
    "diff_df = pd.DataFrame(rows)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# One scatter column per metric\n",
    "metrics_order = res_df[\"metric\"].tolist()  # use same order as stats table\n",
    "positions = {m: i for i, m in enumerate(metrics_order)}\n",
    "\n",
    "for m in metrics_order:\n",
    "    subset = diff_df[diff_df[\"metric\"] == m]\n",
    "    x = np.full(len(subset), positions[m])\n",
    "    plt.scatter(x, subset[\"diff\"], alpha=0.7)\n",
    "\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.xticks(range(len(metrics_order)), metrics_order, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Difference per dataset (DPG - DiCE)\")\n",
    "plt.title(\"Per-dataset metric differences grouped by metric\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "22b2B3bQDwZO",
    "outputId": "2f93b24c-7db1-4a66-9cb7-1a7f5fa1929d"
   },
   "outputs": [],
   "source": [
    "metric_to_plot = \"perc_valid_cf_all\"  # change to the exact base name used in your CSV\n",
    "\n",
    "col_dpg = metric_to_plot + suffix_dpg\n",
    "col_base = metric_to_plot + suffix_baseline\n",
    "\n",
    "x_base = df[col_base].values\n",
    "x_dpg = df[col_dpg].values\n",
    "\n",
    "datasets_labels = df.index.astype(str)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_base, x_dpg)\n",
    "\n",
    "# Diagonal line (equal performance)\n",
    "min_val = min(np.nanmin(x_base), np.nanmin(x_dpg))\n",
    "max_val = max(np.nanmax(x_base), np.nanmax(x_dpg))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\")\n",
    "\n",
    "for xb, xd, label in zip(x_base, x_dpg, datasets_labels):\n",
    "    plt.plot([xb, xb, xd], [xb, xd, xd], alpha=0.3)  # optional segments\n",
    "    # Or annotate points with dataset names (comment if cluttered):\n",
    "    # plt.text(xb, xd, label, fontsize=8)\n",
    "\n",
    "plt.xlabel(\"DiCE\")\n",
    "plt.ylabel(\"DPG-CF\")\n",
    "plt.title(f\"{metric_to_plot}: per-dataset scores (DPG vs DiCE)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "0GAnrsAvD-5X",
    "outputId": "b6cbd9d0-67d5-4c47-fd36-5a9ed7a27dc4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "res_df_plot = res_df.sort_values(\"p_value_raw\")  # or any order you prefer\n",
    "x = np.arange(len(res_df_plot))\n",
    "vals = res_df_plot[\"median_diff(DPG - DiCE)\"].values\n",
    "pvals = res_df_plot[\"p_value_holm\"].values\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(x, vals)\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "\n",
    "for i, (v, p) in enumerate(zip(vals, pvals)):\n",
    "    if p < 0.001:\n",
    "        mark = \"***\"\n",
    "    elif p < 0.01:\n",
    "        mark = \"**\"\n",
    "    elif p < 0.05:\n",
    "        mark = \"*\"\n",
    "    else:\n",
    "        mark = \"\"\n",
    "    if mark:\n",
    "        plt.text(i, v + np.sign(v)*0.01, mark, ha=\"center\", va=\"bottom\" if v>=0 else \"top\")\n",
    "\n",
    "plt.xticks(x, res_df_plot[\"metric\"], rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Median difference (DPG - DiCE)\")\n",
    "plt.title(\"Per-metric median difference with Holm-corrected significance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
