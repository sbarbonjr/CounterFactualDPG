{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DPG vs DiCE Comparison Dashboard\n",
    "\n",
    "This notebook fetches experiment results from WandB and provides interactive visualizations\n",
    "to compare DPG and DiCE counterfactual generation techniques across different datasets.\n",
    "\n",
    "## Features\n",
    "- Fetch and aggregate all experiment runs from WandB\n",
    "- Side-by-side metric comparison tables\n",
    "- Grouped bar charts per metric\n",
    "- Radar charts for dataset-specific profiles\n",
    "- Winner heatmap across all datasets and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import plotly for interactive plots\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"Plotly not available. Install with: pip install plotly\")\n",
    "\n",
    "# Force reload of our comparison module (in case it changed)\n",
    "import importlib\n",
    "import scripts.compare_techniques\n",
    "importlib.reload(scripts.compare_techniques)\n",
    "\n",
    "# Import our comparison module\n",
    "from scripts.compare_techniques import (\n",
    "    fetch_all_runs,\n",
    "    create_comparison_table,\n",
    "    create_method_metrics_table,\n",
    "    print_comparison_summary,\n",
    "    plot_grouped_bar_chart,\n",
    "    plot_radar_chart,\n",
    "    plot_heatmap_winners,\n",
    "    COMPARISON_METRICS,\n",
    "    TECHNIQUE_COLORS,\n",
    "    determine_winner,\n",
    "    filter_to_latest_run_per_combo,\n",
    ")\n",
    "\n",
    "print(\"Imports loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set your WandB project and entity here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# WandB Configuration\n",
    "WANDB_ENTITY = 'mllab-ts-universit-di-trieste'  # Your WandB team/username\n",
    "WANDB_PROJECT = 'CounterFactualDPG'  # Case-sensitive project name\n",
    "\n",
    "# Optional: filter specific datasets (set to None for all)\n",
    "SELECTED_DATASETS = None  # e.g., ['iris', 'german_credit', 'heart_disease_uci']\n",
    "\n",
    "# Toggle: Set to True to load cached data from disk, False to fetch fresh from WandB\n",
    "LOAD_FROM_CACHE = False\n",
    "CACHE_FILE = 'temp/raw_df_cache.pkl'\n",
    "\n",
    "# Optional: Only fetch runs created after this timestamp (set to None for no filter)\n",
    "# Format: ISO 8601 string, e.g., \"2026-01-20T22:00:00\"\n",
    "MIN_CREATED_AT = \"2026-01-26T22:00:00\" \n",
    "\n",
    "# Toggle: Set to True to apply excluded datasets from config.yaml, False to include all datasets\n",
    "APPLY_EXCLUDED_DATASETS = True\n",
    "\n",
    "# Load priority_datasets from main config\n",
    "config_path = '../configs/config.yaml'\n",
    "INCLUDED_DATASETS = None\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        INCLUDED_DATASETS = config.get('priority_datasets', None)\n",
    "\n",
    "print(f\"Entity: {WANDB_ENTITY}\")\n",
    "print(f\"Project: {WANDB_PROJECT}\")\n",
    "print(f\"Datasets filter: {SELECTED_DATASETS or 'All'}\")\n",
    "print(f\"Load from cache: {LOAD_FROM_CACHE}\")\n",
    "print(f\"Min created at: {MIN_CREATED_AT}\")\n",
    "print(f\"Apply excluded datasets: {APPLY_EXCLUDED_DATASETS}\")\n",
    "print(f\"Included datasets: {INCLUDED_DATASETS if INCLUDED_DATASETS else 'All'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Fetch Data from WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "# Load from cache or fetch from WandB\n",
    "if LOAD_FROM_CACHE and os.path.exists(CACHE_FILE):\n",
    "    print(f\"Loading cached data from {CACHE_FILE}...\")\n",
    "    with open(CACHE_FILE, 'rb') as f:\n",
    "        raw_df = pickle.load(f)\n",
    "    print(f\"Loaded {len(raw_df)} runs from cache\")\n",
    "else:\n",
    "    print(\"Fetching data from WandB...\")\n",
    "    raw_df = fetch_all_runs(\n",
    "        project=WANDB_PROJECT,\n",
    "        entity=WANDB_ENTITY,\n",
    "        datasets=SELECTED_DATASETS,\n",
    "        limit=500,\n",
    "        min_created_at=MIN_CREATED_AT\n",
    "    )\n",
    "\n",
    "# Apply included datasets filter if specified\n",
    "if INCLUDED_DATASETS is not None:\n",
    "    pre_filter_count = len(raw_df)\n",
    "    raw_df = raw_df[raw_df['dataset'].isin(INCLUDED_DATASETS)]\n",
    "    print(f\"Applied included datasets filter: {pre_filter_count} -> {len(raw_df)} runs\")\n",
    "    print(f\"Included datasets: {sorted(raw_df['dataset'].unique())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what columns are available in raw_df and verify if cached data is stale\n",
    "print(\"Columns in raw_df:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group columns by type\n",
    "metric_cols = []\n",
    "sample_cols = []\n",
    "other_cols = []\n",
    "\n",
    "for col in sorted(raw_df.columns):\n",
    "    if any(m in col for m in ['perc_valid_cf_', 'perc_actionable_cf_', 'plausibility', 'distance_', 'avg_nbr_', 'count_diversity_', 'accuracy_']):\n",
    "        metric_cols.append(col)\n",
    "    elif col.startswith('sample_'):\n",
    "        sample_cols.append(col)\n",
    "    else:\n",
    "        other_cols.append(col)\n",
    "\n",
    "print(f\"\\nMETRIC COLUMNS (should contain the 7 small metrics):\")\n",
    "for col in sorted(metric_cols):\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nSAMPLE COLUMNS (found {len(sample_cols)}):\")\n",
    "for col in sample_cols[:10]:  # Show first 10\n",
    "    print(f\"  - {col}\")\n",
    "if len(sample_cols) > 10:\n",
    "    print(f\"  ... and {len(sample_cols) - 10} more\")\n",
    "\n",
    "print(f\"\\nOTHER COLUMNS:\")\n",
    "for col in sorted(other_cols):\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total: {len(raw_df.columns)} columns\")\n",
    "\n",
    "# Check if we need to re-fetch from WandB\n",
    "# Check which of the 7 small metrics are present\n",
    "small_metrics_needed = {\n",
    "    'perc_valid_cf_all',\n",
    "    'perc_actionable_cf_all',\n",
    "    'plausibility_nbr_cf',\n",
    "    'distance_mh',\n",
    "    'avg_nbr_changes',\n",
    "    'count_diversity_all',\n",
    "    'accuracy_knn_sklearn'\n",
    "}\n",
    "missing_metrics = [m for m in small_metrics_needed if m not in raw_df.columns]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DIAGNOSTIC: Missing Metrics\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Of the 7 small metrics needed:\")\n",
    "for m in sorted(small_metrics_needed):\n",
    "    status = \"✓ PRESENT\" if m in raw_df.columns else \"✗ MISSING\"\n",
    "    print(f\"  {status}: {m}\")\n",
    "\n",
    "if missing_metrics:\n",
    "    print(f\"\\n⚠️  {len(missing_metrics)} metrics are missing!\")\n",
    "    print(f\"--> This suggests the cached data is stale or was fetched before metrics were logged.\")\n",
    "    print(f\"--> SOLUTION: Set LOAD_FROM_CACHE = False in Cell 5 and re-run Cell 6\")\n",
    "    \n",
    "    # Debug: Check what a sample WandB run actually has\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Checking a sample WandB run to see what metrics are available...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    import wandb\n",
    "    api = wandb.Api()\n",
    "    sample_run = raw_df.iloc[0]\n",
    "    run_id = sample_run['run_id']\n",
    "    print(f\"Fetching run: {run_id} ({sample_run['dataset']}_{sample_run['technique']})\")\n",
    "    \n",
    "    run = api.run(f\"{WANDB_ENTITY}/{WANDB_PROJECT}/{run_id}\")\n",
    "    summary = run.summary._json_dict\n",
    "    \n",
    "    # Look for combination metrics\n",
    "    combo_keys = [k for k in summary.keys() if 'combination' in k.lower() or 'combo' in k.lower()]\n",
    "    print(f\"Found {len(combo_keys)} combination/combo keys in summary:\")\n",
    "    for key in sorted(combo_keys):\n",
    "        print(f\"  - {key}\")\n",
    "    \n",
    "    # Also show some keys with our target metrics\n",
    "    print(f\"\\nSearching for small metrics in summary...\")\n",
    "    for target in list(small_metrics_needed)[:3]:  # Check first 3\n",
    "        matches = [k for k in summary.keys() if target in k]\n",
    "        if matches:\n",
    "            print(f\"  {target}:\")\n",
    "            for m in matches:\n",
    "                print(f\"    - {m}\")\n",
    "else:\n",
    "    print(f\"\\n✓ All 7 small metrics are present in the dataframe!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview raw data\n",
    "display_cols = ['run_id', 'run_name', 'dataset', 'technique', 'state'] + list(COMPARISON_METRICS.keys())[:5]\n",
    "available_cols = [c for c in display_cols if c in raw_df.columns]\n",
    "raw_df[available_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove runs with same method and dataset, keep only latest\n",
    "raw_df = filter_to_latest_run_per_combo(raw_df)  \n",
    "raw_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 3. Create Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregated comparison table\n",
    "comparison_df = create_comparison_table(raw_df, small=True)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris only:\n",
    "\n",
    "iris_df = comparison_df[comparison_df['dataset'] == 'iris']\n",
    "iris_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create method-metrics table for a specific dataset (methods as rows, metrics as columns)\n",
    "# This provides a cleaner view for comparing techniques on a single dataset\n",
    "create_method_metrics_table(raw_df, dataset='iris')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the module to apply the fix for hiding columns\n",
    "import importlib\n",
    "import scripts.compare_techniques\n",
    "importlib.reload(scripts.compare_techniques)\n",
    "\n",
    "from scripts.compare_techniques import (\n",
    "    create_method_metrics_table,\n",
    ")\n",
    "\n",
    "print(\"✓ Module reloaded with fix for 'small' mode!\")\n",
    "print(\"✓ Identical columns will no longer be hidden when small=True\\n\")\n",
    "\n",
    "# Generate method-metrics tables for ALL datasets\n",
    "all_datasets = sorted(raw_df['dataset'].unique())\n",
    "print(f\"Generating tables for {len(all_datasets)} datasets:\\n\")\n",
    "\n",
    "for dataset in all_datasets:\n",
    "    print(f\"Dataset: {dataset.upper()}\")\n",
    "    display(create_method_metrics_table(raw_df, dataset=dataset, small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Dataset Visualizations: DPG vs DiCE\n",
    "\n",
    "Fetch and display visualizations from WandB runs side-by-side. Select a dataset from the dropdown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import re\n",
    "import tempfile\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def fetch_visualizations_from_runs(dpg_run_id: str, dice_run_id: str, entity: str, project: str):\n",
    "    \"\"\"\n",
    "    Fetch visualization images from DPG and DiCE runs.\n",
    "    Returns dict mapping visualization type to (dpg_image, dice_image) tuples,\n",
    "    plus the constraints_overview image (same for both techniques).\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    \n",
    "    dpg_run = api.run(f\"{entity}/{project}/{dpg_run_id}\")\n",
    "    dice_run = api.run(f\"{entity}/{project}/{dice_run_id}\")\n",
    "    \n",
    "    constraints_overview = None\n",
    "    \n",
    "    def get_viz_images(run, capture_constraints=False):\n",
    "        \"\"\"Get visualization images from run, grouped by type.\"\"\"\n",
    "        nonlocal constraints_overview\n",
    "        images = {}\n",
    "        for f in run.files():\n",
    "            if f.name.endswith('.png'):\n",
    "                # Capture constraints_overview from DPG run\n",
    "                if capture_constraints and 'dpg/constraints_overview' in f.name:\n",
    "                    constraints_overview = f\n",
    "                # Check for visualization images\n",
    "                elif 'visualizations/' in f.name:\n",
    "                    basename = os.path.basename(f.name)\n",
    "                    match = re.match(r'([a-z_]+)_\\d+_', basename)\n",
    "                    if match:\n",
    "                        viz_type = match.group(1)\n",
    "                        # Keep only first of each type (they're per-sample)\n",
    "                        if viz_type not in images:\n",
    "                            images[viz_type] = f\n",
    "        return images\n",
    "    \n",
    "    dpg_images = get_viz_images(dpg_run, capture_constraints=True)\n",
    "    dice_images = get_viz_images(dice_run)\n",
    "    \n",
    "    # Find common visualization types\n",
    "    common_types = set(dpg_images.keys()) & set(dice_images.keys())\n",
    "    \n",
    "    result = {}\n",
    "    for viz_type in sorted(common_types):\n",
    "        result[viz_type] = {\n",
    "            'dpg': dpg_images[viz_type],\n",
    "            'dice': dice_images[viz_type]\n",
    "        }\n",
    "    \n",
    "    return result, constraints_overview, dpg_run.name, dice_run.name\n",
    "\n",
    "def download_and_display_comparison(viz_dict: dict, constraints_overview, dpg_name: str, dice_name: str, selected_types: list = None):\n",
    "    \"\"\"\n",
    "    Download images and display them side by side.\n",
    "    First displays the constraints_overview (single image), then side-by-side comparisons.\n",
    "    \"\"\"\n",
    "    # Display constraints overview first (single image, same for both techniques)\n",
    "    if constraints_overview is not None:\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            constraints_overview.download(root=tmpdir, replace=True)\n",
    "            img_path = os.path.join(tmpdir, constraints_overview.name)\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "            ax.imshow(img)\n",
    "            ax.set_title('DPG Constraints Overview', fontsize=16, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    \n",
    "    if selected_types is None:\n",
    "        selected_types = list(viz_dict.keys())\n",
    "    \n",
    "    for viz_type in selected_types:\n",
    "        if viz_type not in viz_dict:\n",
    "            print(f\"Visualization type '{viz_type}' not found. Available: {list(viz_dict.keys())}\")\n",
    "            continue\n",
    "            \n",
    "        dpg_file = viz_dict[viz_type]['dpg']\n",
    "        dice_file = viz_dict[viz_type]['dice']\n",
    "        \n",
    "        # Download images to temp files\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            dpg_file.download(root=tmpdir, replace=True)\n",
    "            dice_file.download(root=tmpdir, replace=True)\n",
    "            \n",
    "            dpg_downloaded = os.path.join(tmpdir, dpg_file.name)\n",
    "            dice_downloaded = os.path.join(tmpdir, dice_file.name)\n",
    "            \n",
    "            # Load and display\n",
    "            dpg_img = Image.open(dpg_downloaded)\n",
    "            dice_img = Image.open(dice_downloaded)\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "            \n",
    "            axes[0].imshow(dpg_img)\n",
    "            axes[0].set_title(f'DPG: {viz_type}', fontsize=14, fontweight='bold')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(dice_img)\n",
    "            axes[1].set_title(f'DiCE: {viz_type}', fontsize=14, fontweight='bold')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            plt.suptitle(f'{viz_type.replace(\"_\", \" \").title()} Comparison', fontsize=16, y=1.02)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "def fetch_dataset_visualizations(dataset_name: str):\n",
    "    \"\"\"Fetch visualizations for a specific dataset.\"\"\"\n",
    "    global viz_dict, constraints_overview, dpg_name, dice_name, selected_dataset\n",
    "    \n",
    "    selected_dataset = dataset_name\n",
    "    dataset_runs = raw_df[raw_df['dataset'] == dataset_name]\n",
    "    \n",
    "    if len(dataset_runs) < 2:\n",
    "        print(f\"Not enough runs found for {dataset_name}\")\n",
    "        return False\n",
    "    \n",
    "    dpg_run = dataset_runs[dataset_runs['technique'] == 'dpg'].iloc[0] if len(dataset_runs[dataset_runs['technique'] == 'dpg']) > 0 else None\n",
    "    dice_run = dataset_runs[dataset_runs['technique'] == 'dice'].iloc[0] if len(dataset_runs[dataset_runs['technique'] == 'dice']) > 0 else None\n",
    "    \n",
    "    if dpg_run is None or dice_run is None:\n",
    "        print(f\"Could not find both DPG and DiCE runs for {dataset_name}\")\n",
    "        return False\n",
    "    \n",
    "    dpg_run_id = dpg_run['run_id']\n",
    "    dice_run_id = dice_run['run_id']\n",
    "    print(f\"Fetching visualizations for {dataset_name}:\")\n",
    "    print(f\"  DPG:  {dpg_run_id} ({dpg_run['run_name']})\")\n",
    "    print(f\"  DiCE: {dice_run_id} ({dice_run['run_name']})\")\n",
    "    \n",
    "    viz_dict, constraints_overview, dpg_name, dice_name = fetch_visualizations_from_runs(\n",
    "        dpg_run_id, dice_run_id, \n",
    "        WANDB_ENTITY, WANDB_PROJECT\n",
    "    )\n",
    "    print(f\"\\nAvailable visualization types: {list(viz_dict.keys())}\")\n",
    "    print(f\"Constraints overview available: {constraints_overview is not None}\")\n",
    "    return True\n",
    "\n",
    "# Get available datasets that have both DPG and DiCE runs\n",
    "available_viz_datasets = []\n",
    "for ds in INCLUDED_DATASETS:\n",
    "    ds_runs = raw_df[raw_df['dataset'] == ds]\n",
    "    has_dpg = len(ds_runs[ds_runs['technique'] == 'dpg']) > 0\n",
    "    has_dice = len(ds_runs[ds_runs['technique'] == 'dice']) > 0\n",
    "    if has_dpg and has_dice:\n",
    "        available_viz_datasets.append(ds)\n",
    "\n",
    "print(f\"Datasets with both DPG and DiCE runs: {available_viz_datasets}\")\n",
    "\n",
    "# Create dropdown widget\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=available_viz_datasets,\n",
    "    value=available_viz_datasets[0] if available_viz_datasets else None,\n",
    "    description='Dataset:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Create fetch button\n",
    "fetch_button = widgets.Button(\n",
    "    description='Fetch Visualizations',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Output area for status messages\n",
    "status_output = widgets.Output()\n",
    "\n",
    "def on_fetch_click(b):\n",
    "    with status_output:\n",
    "        clear_output()\n",
    "        fetch_dataset_visualizations(dataset_dropdown.value)\n",
    "\n",
    "fetch_button.on_click(on_fetch_click)\n",
    "\n",
    "# Display widgets\n",
    "display(widgets.HBox([dataset_dropdown, fetch_button]))\n",
    "display(status_output)\n",
    "\n",
    "# Initialize with first dataset\n",
    "selected_dataset = available_viz_datasets[0] if available_viz_datasets else None\n",
    "viz_dict = {}\n",
    "constraints_overview = None\n",
    "dpg_name = None\n",
    "dice_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display selected visualizations side-by-side for the selected dataset\n",
    "# Choose which visualization types to display (or set to None for all)\n",
    "SELECTED_VIZ_TYPES = ['standard_deviation', 'comparison', 'heatmap', 'pca_clean', 'feature_changes_radar']\n",
    "\n",
    "if 'viz_dict' in dir() and viz_dict:\n",
    "    print(f\"Showing visualizations for: {selected_dataset}\")\n",
    "    print(f\"=\" * 50)\n",
    "    download_and_display_comparison(viz_dict, constraints_overview, dpg_name, dice_name, selected_types=SELECTED_VIZ_TYPES)\n",
    "else:\n",
    "    print(\"Select a dataset and click 'Fetch Visualizations' in the cell above first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed comparison summary\n",
    "print_comparison_summary(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 5. Winner Heatmap\n",
    "\n",
    "This heatmap shows which technique wins for each dataset-metric combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot winner heatmap\n",
    "fig = plot_heatmap_winners(comparison_df, figsize=(16, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 6. Metric-by-Metric Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create bar charts for all metrics\n",
    "# for metric_key, metric_info in COMPARISON_METRICS.items():\n",
    "#     fig = plot_grouped_bar_chart(comparison_df, metric_key, figsize=(14, 6))\n",
    "#     if fig:\n",
    "#         plt.show()\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 7. Dataset-Specific Radar Charts\n",
    "\n",
    "These radar charts show the technique profile for each dataset, making it easy to see strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select a dataset for radar chart\n",
    "# available_datasets = sorted(comparison_df['dataset'].unique())\n",
    "# print(\"Available datasets:\")\n",
    "# for i, ds in enumerate(available_datasets):\n",
    "#     print(f\"  {i}: {ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot radar charts for first 4 datasets (or all if fewer)\n",
    "available_datasets = sorted(comparison_df['dataset'].unique())\n",
    "n_to_show = min(4, len(available_datasets))\n",
    "fig, axes = plt.subplots(1, n_to_show, figsize=(5*n_to_show, 5), subplot_kw=dict(polar=True))\n",
    "\n",
    "if n_to_show == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, dataset in enumerate(available_datasets[:n_to_show]):\n",
    "    # Get data for this dataset\n",
    "    row = comparison_df[comparison_df['dataset'] == dataset].iloc[0]\n",
    "    \n",
    "    metrics = []\n",
    "    dpg_values = []\n",
    "    dice_values = []\n",
    "    \n",
    "    for metric_key, metric_info in COMPARISON_METRICS.items():\n",
    "        dpg_col = f'{metric_key}_dpg'\n",
    "        dice_col = f'{metric_key}_dice'\n",
    "        \n",
    "        if dpg_col in row.index and dice_col in row.index:\n",
    "            dpg_val = row[dpg_col]\n",
    "            dice_val = row[dice_col]\n",
    "            \n",
    "            if pd.notna(dpg_val) and pd.notna(dice_val):\n",
    "                metrics.append(metric_info['name'][:10])  # Truncate for readability\n",
    "                max_val = max(abs(dpg_val), abs(dice_val))\n",
    "                if max_val > 0:\n",
    "                    if metric_info['goal'] == 'minimize':\n",
    "                        dpg_values.append(1 - (dpg_val / (max_val * 1.1)))\n",
    "                        dice_values.append(1 - (dice_val / (max_val * 1.1)))\n",
    "                    else:\n",
    "                        dpg_values.append(dpg_val / (max_val * 1.1))\n",
    "                        dice_values.append(dice_val / (max_val * 1.1))\n",
    "                else:\n",
    "                    dpg_values.append(0.5)\n",
    "                    dice_values.append(0.5)\n",
    "    \n",
    "    if len(metrics) >= 3:\n",
    "        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "        dpg_values += dpg_values[:1]\n",
    "        dice_values += dice_values[:1]\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        ax = axes[i]\n",
    "        ax.plot(angles, dpg_values, 'o-', linewidth=2, label='DPG', color=TECHNIQUE_COLORS['dpg'])\n",
    "        ax.fill(angles, dpg_values, alpha=0.25, color=TECHNIQUE_COLORS['dpg'])\n",
    "        ax.plot(angles, dice_values, 'o-', linewidth=2, label='DiCE', color=TECHNIQUE_COLORS['dice'])\n",
    "        ax.fill(angles, dice_values, alpha=0.25, color=TECHNIQUE_COLORS['dice'])\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(metrics, size=8)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title(dataset, size=12)\n",
    "        if i == 0:\n",
    "            ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "plt.suptitle('Dataset Comparison Profiles (Higher = Better)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 8. Interactive Plotly Visualizations (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOTLY_AVAILABLE:\n",
    "    # Create interactive grouped bar chart\n",
    "    datasets = sorted(comparison_df['dataset'].unique())\n",
    "    \n",
    "    # Prepare data for a specific metric\n",
    "    metric_key = 'perc_valid_cf'  # Change this to view other metrics\n",
    "    metric_info = COMPARISON_METRICS[metric_key]\n",
    "    \n",
    "    dpg_col = f'{metric_key}_dpg'\n",
    "    dice_col = f'{metric_key}_dice'\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name='DPG',\n",
    "        x=comparison_df['dataset'],\n",
    "        y=comparison_df[dpg_col],\n",
    "        marker_color=TECHNIQUE_COLORS['dpg'],\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name='DiCE',\n",
    "        x=comparison_df['dataset'],\n",
    "        y=comparison_df[dice_col],\n",
    "        marker_color=TECHNIQUE_COLORS['dice'],\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"{metric_info['name']}: DPG vs DiCE\",\n",
    "        xaxis_title='Dataset',\n",
    "        yaxis_title=metric_info['name'],\n",
    "        barmode='group',\n",
    "        xaxis_tickangle=-45,\n",
    "        height=500,\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Plotly not available for interactive charts. Install with: pip install plotly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 9. Cross-Dataset Summary\n",
    "\n",
    "Aggregated view of technique performance across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall win rates\n",
    "win_counts = {'dpg': {}, 'dice': {}}\n",
    "\n",
    "for metric_key, metric_info in COMPARISON_METRICS.items():\n",
    "    dpg_col = f'{metric_key}_dpg'\n",
    "    dice_col = f'{metric_key}_dice'\n",
    "    \n",
    "    if dpg_col not in comparison_df.columns or dice_col not in comparison_df.columns:\n",
    "        continue\n",
    "    \n",
    "    dpg_wins = 0\n",
    "    dice_wins = 0\n",
    "    \n",
    "    for _, row in comparison_df.iterrows():\n",
    "        winner = determine_winner(row.get(dpg_col), row.get(dice_col), metric_info['goal'])\n",
    "        if winner == 'dpg':\n",
    "            dpg_wins += 1\n",
    "        elif winner == 'dice':\n",
    "            dice_wins += 1\n",
    "    \n",
    "    win_counts['dpg'][metric_key] = dpg_wins\n",
    "    win_counts['dice'][metric_key] = dice_wins\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for metric_key, metric_info in COMPARISON_METRICS.items():\n",
    "    dpg_w = win_counts['dpg'].get(metric_key, 0)\n",
    "    dice_w = win_counts['dice'].get(metric_key, 0)\n",
    "    total = dpg_w + dice_w\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Metric': metric_info['name'],\n",
    "        'Goal': metric_info['goal'],\n",
    "        'DPG Wins': dpg_w,\n",
    "        'DiCE Wins': dice_w,\n",
    "        'DPG Win Rate': f\"{dpg_w/total*100:.1f}%\" if total > 0 else \"N/A\",\n",
    "        'Better': 'DPG' if dpg_w > dice_w else ('DiCE' if dice_w > dpg_w else 'Tie'),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.style.apply(\n",
    "    lambda x: ['background-color: #d4edda' if v == 'DPG' else \n",
    "               'background-color: #cce5ff' if v == 'DiCE' else '' \n",
    "               for v in x], \n",
    "    subset=['Better']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save comparison table to CSV\n",
    "# output_path = '../outputs/technique_comparison.csv'\n",
    "# comparison_df.to_csv(output_path, index=False)\n",
    "# print(f\"Comparison data saved to: {output_path}\")\n",
    "\n",
    "# # Save summary statistics\n",
    "# summary_path = '../outputs/technique_summary.csv'\n",
    "# summary_df.to_csv(summary_path, index=False)\n",
    "# print(f\"Summary statistics saved to: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
