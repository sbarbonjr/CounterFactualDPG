{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DPG vs DiCE Comparison Dashboard\n",
    "\n",
    "This notebook fetches experiment results from WandB and provides interactive visualizations\n",
    "to compare DPG and DiCE counterfactual generation techniques across different datasets.\n",
    "\n",
    "## Features\n",
    "- Fetch and aggregate all experiment runs from WandB\n",
    "- Side-by-side metric comparison tables\n",
    "- Grouped bar charts per metric\n",
    "- Radar charts for dataset-specific profiles\n",
    "- Winner heatmap across all datasets and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import plotly for interactive plots\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"Plotly not available. Install with: pip install plotly\")\n",
    "\n",
    "# Force reload of our comparison module (in case it changed)\n",
    "import importlib\n",
    "import scripts.compare_techniques\n",
    "importlib.reload(scripts.compare_techniques)\n",
    "\n",
    "# Import our comparison module\n",
    "from scripts.compare_techniques import (\n",
    "    fetch_all_runs,\n",
    "    create_comparison_table,\n",
    "    create_method_metrics_table,\n",
    "    print_comparison_summary,\n",
    "    plot_grouped_bar_chart,\n",
    "    plot_radar_chart,\n",
    "    plot_heatmap_winners,\n",
    "    COMPARISON_METRICS,\n",
    "    TECHNIQUE_COLORS,\n",
    "    determine_winner,\n",
    "    filter_to_latest_run_per_combo,\n",
    ")\n",
    "\n",
    "print(\"Imports loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set your WandB project and entity here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB Configuration\n",
    "WANDB_ENTITY = 'mllab-ts-universit-di-trieste'  # Your WandB team/username\n",
    "WANDB_PROJECT = 'CounterFactualDPG'  # Case-sensitive project name\n",
    "\n",
    "# Optional: filter specific datasets (set to None for all)\n",
    "SELECTED_DATASETS = None  # e.g., ['iris', 'german_credit', 'heart_disease_uci']\n",
    "\n",
    "# Toggle: Set to True to load cached data from disk, False to fetch fresh from WandB\n",
    "LOAD_FROM_CACHE = False\n",
    "CACHE_FILE = 'temp/raw_df_cache.pkl'\n",
    "\n",
    "# Optional: Only fetch runs created after this timestamp (set to None for no filter)\n",
    "# Format: ISO 8601 string, e.g., \"2026-01-20T22:00:00\"\n",
    "MIN_CREATED_AT = \"2026-01-21T22:00:00\" \n",
    "\n",
    "# Toggle: Set to True to apply excluded datasets from config.yaml, False to include all datasets\n",
    "APPLY_EXCLUDED_DATASETS = True\n",
    "\n",
    "print(f\"Entity: {WANDB_ENTITY}\")\n",
    "print(f\"Project: {WANDB_PROJECT}\")\n",
    "print(f\"Datasets filter: {SELECTED_DATASETS or 'All'}\")\n",
    "print(f\"Load from cache: {LOAD_FROM_CACHE}\")\n",
    "print(f\"Min created at: {MIN_CREATED_AT}\")\n",
    "print(f\"Apply excluded datasets: {APPLY_EXCLUDED_DATASETS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Fetch Data from WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "# Load from cache or fetch from WandB\n",
    "if LOAD_FROM_CACHE and os.path.exists(CACHE_FILE):\n",
    "    print(f\"Loading cached data from {CACHE_FILE}...\")\n",
    "    with open(CACHE_FILE, 'rb') as f:\n",
    "        raw_df = pickle.load(f)\n",
    "    print(f\"Loaded {len(raw_df)} runs from cache\")\n",
    "else:\n",
    "    print(\"Fetching data from WandB...\")\n",
    "    raw_df = fetch_all_runs(\n",
    "        project=WANDB_PROJECT,\n",
    "        entity=WANDB_ENTITY,\n",
    "        datasets=SELECTED_DATASETS,\n",
    "        limit=500,\n",
    "        min_created_at=MIN_CREATED_AT\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview raw data\n",
    "display_cols = ['run_name', 'dataset', 'technique', 'state'] + list(COMPARISON_METRICS.keys())[:5]\n",
    "available_cols = [c for c in display_cols if c in raw_df.columns]\n",
    "raw_df[available_cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Create Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregated comparison table\n",
    "comparison_df = create_comparison_table(raw_df)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris only:\n",
    "\n",
    "iris_df = comparison_df[comparison_df['dataset'] == 'iris']\n",
    "iris_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create method-metrics table for a specific dataset (methods as rows, metrics as columns)\n",
    "# This provides a cleaner view for comparing techniques on a single dataset\n",
    "create_method_metrics_table(raw_df, dataset='iris')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate method-metrics tables for ALL datasets\n",
    "all_datasets = sorted(raw_df['dataset'].unique())\n",
    "print(f\"Generating tables for {len(all_datasets)} datasets:\\n\")\n",
    "\n",
    "for dataset in all_datasets:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset: {dataset.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    display(create_method_metrics_table(raw_df, dataset=dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed comparison summary\n",
    "print_comparison_summary(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 5. Winner Heatmap\n",
    "\n",
    "This heatmap shows which technique wins for each dataset-metric combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot winner heatmap\n",
    "fig = plot_heatmap_winners(comparison_df, figsize=(16, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 6. Metric-by-Metric Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar charts for all metrics\n",
    "for metric_key, metric_info in COMPARISON_METRICS.items():\n",
    "    fig = plot_grouped_bar_chart(comparison_df, metric_key, figsize=(14, 6))\n",
    "    if fig:\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 7. Dataset-Specific Radar Charts\n",
    "\n",
    "These radar charts show the technique profile for each dataset, making it easy to see strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset for radar chart\n",
    "available_datasets = sorted(comparison_df['dataset'].unique())\n",
    "print(\"Available datasets:\")\n",
    "for i, ds in enumerate(available_datasets):\n",
    "    print(f\"  {i}: {ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot radar charts for first 4 datasets (or all if fewer)\n",
    "n_to_show = min(4, len(available_datasets))\n",
    "fig, axes = plt.subplots(1, n_to_show, figsize=(5*n_to_show, 5), subplot_kw=dict(polar=True))\n",
    "\n",
    "if n_to_show == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, dataset in enumerate(available_datasets[:n_to_show]):\n",
    "    # Get data for this dataset\n",
    "    row = comparison_df[comparison_df['dataset'] == dataset].iloc[0]\n",
    "    \n",
    "    metrics = []\n",
    "    dpg_values = []\n",
    "    dice_values = []\n",
    "    \n",
    "    for metric_key, metric_info in COMPARISON_METRICS.items():\n",
    "        dpg_col = f'{metric_key}_dpg'\n",
    "        dice_col = f'{metric_key}_dice'\n",
    "        \n",
    "        if dpg_col in row.index and dice_col in row.index:\n",
    "            dpg_val = row[dpg_col]\n",
    "            dice_val = row[dice_col]\n",
    "            \n",
    "            if pd.notna(dpg_val) and pd.notna(dice_val):\n",
    "                metrics.append(metric_info['name'][:10])  # Truncate for readability\n",
    "                max_val = max(abs(dpg_val), abs(dice_val))\n",
    "                if max_val > 0:\n",
    "                    if metric_info['goal'] == 'minimize':\n",
    "                        dpg_values.append(1 - (dpg_val / (max_val * 1.1)))\n",
    "                        dice_values.append(1 - (dice_val / (max_val * 1.1)))\n",
    "                    else:\n",
    "                        dpg_values.append(dpg_val / (max_val * 1.1))\n",
    "                        dice_values.append(dice_val / (max_val * 1.1))\n",
    "                else:\n",
    "                    dpg_values.append(0.5)\n",
    "                    dice_values.append(0.5)\n",
    "    \n",
    "    if len(metrics) >= 3:\n",
    "        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "        dpg_values += dpg_values[:1]\n",
    "        dice_values += dice_values[:1]\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        ax = axes[i]\n",
    "        ax.plot(angles, dpg_values, 'o-', linewidth=2, label='DPG', color=TECHNIQUE_COLORS['dpg'])\n",
    "        ax.fill(angles, dpg_values, alpha=0.25, color=TECHNIQUE_COLORS['dpg'])\n",
    "        ax.plot(angles, dice_values, 'o-', linewidth=2, label='DiCE', color=TECHNIQUE_COLORS['dice'])\n",
    "        ax.fill(angles, dice_values, alpha=0.25, color=TECHNIQUE_COLORS['dice'])\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(metrics, size=8)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title(dataset, size=12)\n",
    "        if i == 0:\n",
    "            ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "plt.suptitle('Dataset Comparison Profiles (Higher = Better)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 8. Interactive Plotly Visualizations (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOTLY_AVAILABLE:\n",
    "    # Create interactive grouped bar chart\n",
    "    datasets = sorted(comparison_df['dataset'].unique())\n",
    "    \n",
    "    # Prepare data for a specific metric\n",
    "    metric_key = 'perc_valid_cf'  # Change this to view other metrics\n",
    "    metric_info = COMPARISON_METRICS[metric_key]\n",
    "    \n",
    "    dpg_col = f'{metric_key}_dpg'\n",
    "    dice_col = f'{metric_key}_dice'\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name='DPG',\n",
    "        x=comparison_df['dataset'],\n",
    "        y=comparison_df[dpg_col],\n",
    "        marker_color=TECHNIQUE_COLORS['dpg'],\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name='DiCE',\n",
    "        x=comparison_df['dataset'],\n",
    "        y=comparison_df[dice_col],\n",
    "        marker_color=TECHNIQUE_COLORS['dice'],\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"{metric_info['name']}: DPG vs DiCE\",\n",
    "        xaxis_title='Dataset',\n",
    "        yaxis_title=metric_info['name'],\n",
    "        barmode='group',\n",
    "        xaxis_tickangle=-45,\n",
    "        height=500,\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Plotly not available for interactive charts. Install with: pip install plotly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 9. Cross-Dataset Summary\n",
    "\n",
    "Aggregated view of technique performance across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall win rates\n",
    "win_counts = {'dpg': {}, 'dice': {}}\n",
    "\n",
    "for metric_key, metric_info in COMPARISON_METRICS.items():\n",
    "    dpg_col = f'{metric_key}_dpg'\n",
    "    dice_col = f'{metric_key}_dice'\n",
    "    \n",
    "    if dpg_col not in comparison_df.columns or dice_col not in comparison_df.columns:\n",
    "        continue\n",
    "    \n",
    "    dpg_wins = 0\n",
    "    dice_wins = 0\n",
    "    \n",
    "    for _, row in comparison_df.iterrows():\n",
    "        winner = determine_winner(row.get(dpg_col), row.get(dice_col), metric_info['goal'])\n",
    "        if winner == 'dpg':\n",
    "            dpg_wins += 1\n",
    "        elif winner == 'dice':\n",
    "            dice_wins += 1\n",
    "    \n",
    "    win_counts['dpg'][metric_key] = dpg_wins\n",
    "    win_counts['dice'][metric_key] = dice_wins\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for metric_key, metric_info in COMPARISON_METRICS.items():\n",
    "    dpg_w = win_counts['dpg'].get(metric_key, 0)\n",
    "    dice_w = win_counts['dice'].get(metric_key, 0)\n",
    "    total = dpg_w + dice_w\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Metric': metric_info['name'],\n",
    "        'Goal': metric_info['goal'],\n",
    "        'DPG Wins': dpg_w,\n",
    "        'DiCE Wins': dice_w,\n",
    "        'DPG Win Rate': f\"{dpg_w/total*100:.1f}%\" if total > 0 else \"N/A\",\n",
    "        'Better': 'DPG' if dpg_w > dice_w else ('DiCE' if dice_w > dpg_w else 'Tie'),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.style.apply(\n",
    "    lambda x: ['background-color: #d4edda' if v == 'DPG' else \n",
    "               'background-color: #cce5ff' if v == 'DiCE' else '' \n",
    "               for v in x], \n",
    "    subset=['Better']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison table to CSV\n",
    "output_path = '../outputs/technique_comparison.csv'\n",
    "comparison_df.to_csv(output_path, index=False)\n",
    "print(f\"Comparison data saved to: {output_path}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_path = '../outputs/technique_summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"Summary statistics saved to: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
