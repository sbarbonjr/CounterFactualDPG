{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Compare Two WandB Runs Side by Side\n",
    "\n",
    "This notebook fetches and visualizes data from two experiment runs for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional, List\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Import wandb\n",
    "import wandb\n",
    "from wandb.apis.public import Run\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Experiment Tracking - Define Run IDs\n",
    "RUN_IDS = ['poxmea6n', 'buqt4b6u']\n",
    "ENTITY = 'mllab-ts-universit-di-trieste'\n",
    "PROJECT = 'CounterFactualDPG'\n",
    "\n",
    "print(f\"Comparing {len(RUN_IDS)} runs:\")\n",
    "for i, run_id in enumerate(RUN_IDS, 1):\n",
    "    print(f\"  {i}. {run_id}\")\n",
    "print(f\"\\nEntity: {ENTITY}\")\n",
    "print(f\"Project: {PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Run Data from WandB\n",
    "api = wandb.Api()\n",
    "\n",
    "def fetch_run_data(run_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Fetch comprehensive information from a WandB run.\"\"\"\n",
    "    run_path = f\"{ENTITY}/{PROJECT}/{run_id}\"\n",
    "    print(f\"Fetching run: {run_path}\")\n",
    "    \n",
    "    try:\n",
    "        run = api.run(run_path)\n",
    "        \n",
    "        # Collect all run information\n",
    "        run_data = {\n",
    "            'meta': {\n",
    "                'id': run.id,\n",
    "                'name': run.name,\n",
    "                'display_name': run.display_name,\n",
    "                'state': run.state,\n",
    "                'url': run.url,\n",
    "                'path': run.path,\n",
    "                'entity': run.entity,\n",
    "                'project': run.project,\n",
    "                'created_at': run.created_at,\n",
    "                'updated_at': getattr(run, 'updated_at', None),\n",
    "                'notes': run.notes,\n",
    "                'tags': list(run.tags) if run.tags else [],\n",
    "                'group': run.group,\n",
    "                'job_type': run.job_type,\n",
    "            },\n",
    "            'config': dict(run.config),\n",
    "            'summary': {},\n",
    "            'history': [],\n",
    "            'history_keys': [],\n",
    "            'system_metrics': {},\n",
    "            'files': [],\n",
    "            'artifacts': [],\n",
    "        }\n",
    "        \n",
    "        # Get summary metrics\n",
    "        for key, value in run.summary.items():\n",
    "            if not key.startswith('_'):\n",
    "                try:\n",
    "                    run_data['summary'][key] = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    run_data['summary'][key] = value\n",
    "        \n",
    "        # Get history (time-series data)\n",
    "        try:\n",
    "            history = run.history(pandas=False)\n",
    "            if history:\n",
    "                run_data['history'] = list(history)\n",
    "                # Extract unique keys from history\n",
    "                all_keys = set()\n",
    "                for row in history:\n",
    "                    all_keys.update(row.keys())\n",
    "                run_data['history_keys'] = sorted(list(all_keys))\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not fetch history: {e}\")\n",
    "        \n",
    "        # Get files\n",
    "        try:\n",
    "            files = run.files()\n",
    "            run_data['files'] = [\n",
    "                {\n",
    "                    'name': f.name,\n",
    "                    'size': f.size,\n",
    "                    'mimetype': getattr(f, 'mimetype', None),\n",
    "                    'url': f.url,\n",
    "                }\n",
    "                for f in files\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not fetch files: {e}\")\n",
    "        \n",
    "        # Get artifacts\n",
    "        try:\n",
    "            artifacts = run.logged_artifacts()\n",
    "            run_data['artifacts'] = [\n",
    "                {\n",
    "                    'name': a.name,\n",
    "                    'type': a.type,\n",
    "                    'version': a.version,\n",
    "                    'size': a.size,\n",
    "                }\n",
    "                for a in artifacts\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not fetch artifacts: {e}\")\n",
    "        \n",
    "        print(f\"  ‚úì Successfully fetched {len(run_data['history'])} history steps\")\n",
    "        return run_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error fetching run: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch data for all runs\n",
    "runs_data = {}\n",
    "for run_id in RUN_IDS:\n",
    "    runs_data[run_id] = fetch_run_data(run_id)\n",
    "\n",
    "print(f\"\\n‚úì Loaded data for {len([r for r in runs_data.values() if r])}/{len(RUN_IDS)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Metrics and Parameters for Comparison\n",
    "def format_value(value, max_length=50):\n",
    "    \"\"\"Format values for display.\"\"\"\n",
    "    if value is None:\n",
    "        return \"N/A\"\n",
    "    if isinstance(value, (list, dict)):\n",
    "        return f\"{type(value).__name__}({len(value)})\"\n",
    "    str_val = str(value)\n",
    "    if len(str_val) > max_length:\n",
    "        return str_val[:max_length-3] + \"...\"\n",
    "    return str_val\n",
    "\n",
    "# Create comparison DataFrames\n",
    "comparison_data = {\n",
    "    'Metric': [f\"Run {i+1} ({run_id})\" for i, run_id in enumerate(RUN_IDS)]\n",
    "}\n",
    "for run_id in RUN_IDS:\n",
    "    comparison_data[f\"Run {RUN_IDS.index(run_id)+1} ({run_id})\"] = []\n",
    "\n",
    "# Metadata comparison\n",
    "meta_fields = ['name', 'state', 'created_at', 'tags', 'group', 'job_type']\n",
    "for field in meta_fields:\n",
    "    comparison_data['Metric'].append(f\"meta.{field}\")\n",
    "    for run_id in RUN_IDS:\n",
    "        comparison_data[f\"Run {RUN_IDS.index(run_id)+1} ({run_id})\"].append(format_value(runs_data[run_id]['meta'].get(field)))\n",
    "\n",
    "# Summary metrics - find common keys\n",
    "summary_keys_1 = set(runs_data['poxmea6n']['summary'].keys())\n",
    "summary_keys_2 = set(runs_data['buqt4b6u']['summary'].keys())\n",
    "common_summary_keys = sorted(summary_keys_1 & summary_keys_2)\n",
    "\n",
    "for key in common_summary_keys:\n",
    "    comparison_data['Metric'].append(f\"summary.{key}\")\n",
    "    comparison_data['Run 1 (poxmea6n)'].append(format_value(runs_data['poxmea6n']['summary'].get(key)))\n",
    "    comparison_data['Run 2 (buqt4b6u)'].append(format_value(runs_data['buqt4b6u']['summary'].get(key)))\n",
    "\n",
    "# Configuration comparison\n",
    "config_keys_1 = set(runs_data['poxmea6n']['config'].keys())\n",
    "config_keys_2 = set(runs_data['buqt4b6u']['config'].keys())\n",
    "common_config_keys = sorted(config_keys_1 & config_keys_2)\n",
    "\n",
    "for key in common_config_keys:\n",
    "    comparison_data['Metric'].append(f\"config.{key}\")\n",
    "    comparison_data['Run 1 (poxmea6n)'].append(format_value(runs_data['poxmea6n']['config'].get(key)))\n",
    "    comparison_data['Run 2 (buqt4b6u)'].append(format_value(runs_data['buqt4b6u']['config'].get(key)))\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"‚úì Extracted metrics and parameters for comparison\")\n",
    "display(df_comparison.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Side-by-Side Comparison Tables\n",
    "\n",
    "# 1. Metadata Comparison\n",
    "meta_data = []\n",
    "for run_id in RUN_IDS:\n",
    "    meta_data.append(runs_data[run_id]['meta'])\n",
    "\n",
    "df_meta = pd.DataFrame(meta_data, index=['Run 1 (poxmea6n)', 'Run 2 (buqt4b6u)']).T\n",
    "\n",
    "# 2. Summary Metrics Comparison\n",
    "df_summary = pd.DataFrame({\n",
    "    'Run 1 (poxmea6n)': runs_data['poxmea6n']['summary'],\n",
    "    'Run 2 (buqt4b6u)': runs_data['buqt4b6u']['summary']\n",
    "})\n",
    "\n",
    "# 3. Configuration Comparison\n",
    "df_config = pd.DataFrame({\n",
    "    'Run 1 (poxmea6n)': runs_data['poxmea6n']['config'],\n",
    "    'Run 2 (buqt4b6u)': runs_data['buqt4b6u']['config']\n",
    "})\n",
    "\n",
    "# Display side by side\n",
    "HTML(\"\"\"\n",
    "<div style=\"display: flex; gap: 20px; overflow-x: auto;\">\n",
    "    <div style=\"flex: 1;\">\n",
    "        <h3 style=\"color: #2e86de; margin-bottom: 10px;\">üìã Run Metadata</h3>\n",
    "        {meta_html}\n",
    "    </div>\n",
    "    <div style=\"flex: 1;\">\n",
    "        <h3 style=\"color: #10ac84; margin-bottom: 10px;\">üìä Summary Metrics</h3>\n",
    "        {summary_html}\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\".format(\n",
    "    meta_html=df_meta.to_html(classes=\"data-table\"),\n",
    "    summary_html=df_summary.head(20).to_html(classes=\"data-table\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training History - Side by Side\n",
    "\n",
    "# Find common history keys\n",
    "history_keys_1 = set(runs_data['poxmea6n']['history_keys'])\n",
    "history_keys_2 = set(runs_data['buqt4b6u']['history_keys'])\n",
    "common_metrics = sorted(history_keys_1 & history_keys_2)\n",
    "\n",
    "print(f\"Common metrics found in history: {len(common_metrics)}\")\n",
    "print(f\"Metrics: {common_metrics}\")\n",
    "\n",
    "# Filter to numeric metrics\n",
    "numeric_metrics = []\n",
    "for key in common_metrics:\n",
    "    # Check if this is a numeric metric\n",
    "    is_numeric = True\n",
    "    for run_id in RUN_IDS:\n",
    "        for row in runs_data[run_id]['history']:\n",
    "            if key in row:\n",
    "                val = row[key]\n",
    "                if val is not None and not isinstance(val, (int, float)):\n",
    "                    is_numeric = False\n",
    "                    break\n",
    "    if is_numeric and any(key in row for row in runs_data['poxmea6n']['history']):\n",
    "        numeric_metrics.append(key)\n",
    "\n",
    "print(f\"\\nNumeric metrics to plot: {numeric_metrics}\")\n",
    "\n",
    "# Create subplots for each metric\n",
    "if numeric_metrics:\n",
    "    n_metrics = min(len(numeric_metrics), 8)  # Limit to 8 metrics\n",
    "    n_cols = 2\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=n_rows, cols=n_cols,\n",
    "        subplot_titles=numeric_metrics[:n_metrics]\n",
    "    )\n",
    "    \n",
    "    colors = ['#2e86de', '#10ac84']\n",
    "    \n",
    "    for idx, metric in enumerate(numeric_metrics[:n_metrics]):\n",
    "        row = (idx // n_cols) + 1\n",
    "        col = (idx % n_cols) + 1\n",
    "        \n",
    "        for i, run_id in enumerate(RUN_IDS):\n",
    "            history = runs_data[run_id]['history']\n",
    "            steps = []\n",
    "            values = []\n",
    "            for row_data in history:\n",
    "                if metric in row_data:\n",
    "                    steps.append(row_data.get('_step', len(values)))\n",
    "                    values.append(row_data[metric])\n",
    "            \n",
    "            if steps and values:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=steps,\n",
    "                        y=values,\n",
    "                        mode='lines+markers',\n",
    "                        name=f\"Run {i+1} ({run_id})\",\n",
    "                        legendgroup=f\"group_{i}\" if idx == 0 else None,\n",
    "                        showlegend=(idx == 0),\n",
    "                        line=dict(color=colors[i], width=2),\n",
    "                        marker=dict(size=4)\n",
    "                    ),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=300 * n_rows,\n",
    "        title_text=\"<b>Training History: Side-by-Side Comparison</b>\",\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No numeric metrics found in history to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Model Performance - Summary Metrics Bar Chart\n",
    "\n",
    "# Extract numeric summary metrics for comparison\n",
    "metric_values = []\n",
    "metric_names = []\n",
    "\n",
    "for key in runs_data['poxmea6n']['summary'].keys():\n",
    "    if not key.startswith('_') and isinstance(runs_data['poxmea6n']['summary'].get(key), (int, float)):\n",
    "        try:\n",
    "            val1 = float(runs_data['poxmea6n']['summary'][key])\n",
    "            val2 = float(runs_data['buqt4b6u'].get('summary', {}).get(key, 0))\n",
    "            \n",
    "            # Filter out extreme values for better visualization\n",
    "            if abs(val1) < 1e6 and abs(val2) < 1e6:\n",
    "                metric_names.append(key)\n",
    "                metric_values.append([val1, val2])\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "if metric_values:\n",
    "    df_metrics = pd.DataFrame(metric_values, columns=['Run 1 (poxmea6n)', 'Run 2 (buqt4b6u)'], index=metric_names)\n",
    "    \n",
    "    # Bar chart comparison\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Run 1 (poxmea6n)',\n",
    "        x=metric_names,\n",
    "        y=df_metrics['Run 1 (poxmea6n)'],\n",
    "        marker_color='#2e86de'\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Run 2 (buqt4b6u)',\n",
    "        x=metric_names,\n",
    "        y=df_metrics['Run 2 (buqt4b6u)'],\n",
    "        marker_color='#10ac84'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='<b>Summary Metrics Comparison</b>',\n",
    "        xaxis_title='Metric',\n",
    "        yaxis_title='Value',\n",
    "        barmode='group',\n",
    "        height=600,\n",
    "        hovermode='x unified',\n",
    "        xaxis={'tickangle': -45},\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No numeric summary metrics found for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Difference Analysis\n",
    "\n",
    "# Calculate percentage differences\n",
    "differences = []\n",
    "for key in runs_data['poxmea6n']['summary'].keys():\n",
    "    if (not key.startswith('_') and \n",
    "        key in runs_data['buqt4b6u']['summary'] and\n",
    "        isinstance(runs_data['poxmea6n']['summary'].get(key), (int, float)) and\n",
    "        isinstance(runs_data['buqt4b6u']['summary'].get(key), (int, float))):\n",
    "        \n",
    "        try:\n",
    "            val1 = float(runs_data['poxmea6n']['summary'][key])\n",
    "            val2 = float(runs_data['buqt4b6u']['summary'][key])\n",
    "            \n",
    "            if abs(val1) > 1e-10:  # Avoid division by zero\n",
    "                pct_diff = ((val2 - val1) / abs(val1)) * 100\n",
    "                absolute_diff = val2 - val1\n",
    "                \n",
    "                # Filter for meaningful differences\n",
    "                if abs(val1) < 1e6 and abs(val2) < 1e6:\n",
    "                    differences.append({\n",
    "                        'Metric': key,\n",
    "                        'Run 1': val1,\n",
    "                        'Run 2': val2,\n",
    "                        'Absolute Diff': absolute_diff,\n",
    "                        'Percent Diff (%)': pct_diff,\n",
    "                        'Better Run': 'Run 2' if pct_diff > 0 else 'Run 1'\n",
    "                    })\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "if differences:\n",
    "    df_diff = pd.DataFrame(differences).sort_values('Absolute Diff', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"Top Metric Differences:\")\n",
    "    df_diff_top = df_diff.head(15)\n",
    "    display(df_diff_top)\n",
    "    \n",
    "    # Visualization of differences\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Color based on whether Run 2 improved\n",
    "    colors = ['#10ac84' if diff > 0 else '#ee5253' for diff in df_diff_top['Percent Diff (%)']]\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_diff_top['Metric'],\n",
    "        y=df_diff_top['Percent Diff (%)'],\n",
    "        marker_color=colors,\n",
    "        hovertemplate='%{x}<br>Percent Change: %{y:.2f}%<br>Run 1: %{customdata[0]:.4f}<br>Run 2: %{customdata[1]:.4f}<extra></extra>',\n",
    "        customdata=zip(df_diff_top['Run 1'], df_diff_top['Run 2'])\n",
    "    ))\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", opacity=0.5)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='<b>Percentage Difference: Run 2 vs Run 1</b>',\n",
    "        xaxis_title='Metric',\n",
    "        yaxis_title='Percentage Difference (%)',\n",
    "        height=600,\n",
    "        xaxis={'tickangle': -45},\n",
    "        template='plotly_white',\n",
    "        annotations=[\n",
    "            dict(x=0.02, y=0.98, \n",
    "                 text='<span style=\"color:#10ac84\">‚óè Run 2 better</span> <br> <span style=\"color:#ee5253\">‚óè Run 1 better</span>',\n",
    "                 showarrow=False, xref='paper', yref='paper', \n",
    "                 xanchor='left', yanchor='top', bgcolor='rgba(255,255,255,0.9)',\n",
    "                 bordercolor='gray', borderwidth=1)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No comparable metrics found for difference analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Files and Artifacts\n",
    "\n",
    "print(\"üìÅ Files Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "files_data = {'File Name': [], 'Run 1 Size (KB)': [], 'Run 2 Size (KB)': [], 'Available In': []}\n",
    "\n",
    "# Get all unique file names\n",
    "all_files = set()\n",
    "for run_id in RUN_IDS:\n",
    "    for f in runs_data[run_id]['files']:\n",
    "        all_files.add(f['name'])\n",
    "\n",
    "for filename in sorted(all_files):\n",
    "    available_in = []\n",
    "    size1 = \"N/A\"\n",
    "    size2 = \"N/A\"\n",
    "    \n",
    "    if filename in [f['name'] for f in runs_data['poxmea6n']['files']]:\n",
    "        available_in.append('Run 1')\n",
    "        f = next(f for f in runs_data['poxmea6n']['files'] if f['name'] == filename)\n",
    "        size1 = f\"{f['size'] / 1024:.2f}\" if f['size'] else \"Unknown\"\n",
    "    \n",
    "    if filename in [f['name'] for f in runs_data['buqt4b6u']['files']]:\n",
    "        available_in.append('Run 2')\n",
    "        f = next(f for f in runs_data['buqt4b6u']['files'] if f['name'] == filename)\n",
    "        size2 = f\"{f['size'] / 1024:.2f}\" if f['size'] else \"Unknown\"\n",
    "    \n",
    "    files_data['File Name'].append(filename)\n",
    "    files_data['Run 1 Size (KB)'].append(size1)\n",
    "    files_data['Run 2 Size (KB)'].append(size2)\n",
    "    files_data['Available In'].append(', '.join(available_in))\n",
    "\n",
    "df_files = pd.DataFrame(files_data)\n",
    "display(df_files.head(20))\n",
    "\n",
    "print(\"\\nüì¶ Artifacts Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "artifacts_data = {'Artifact': [], 'Run 1 Version': [], 'Run 2 Version': [], 'Available In': [], 'Type': []}\n",
    "\n",
    "# Get all unique artifact names\n",
    "all_artifacts = set()\n",
    "for run_id in RUN_IDS:\n",
    "    for a in runs_data[run_id]['artifacts']:\n",
    "        all_artifacts.add(a['name'])\n",
    "\n",
    "for artifact_name in sorted(all_artifacts):\n",
    "    available_in = []\n",
    "    version1 = \"N/A\"\n",
    "    version2 = \"N/A\"\n",
    "    artifact_type = \"N/A\"\n",
    "    \n",
    "    if artifact_name in [a['name'] for a in runs_data['poxmea6n']['artifacts']]:\n",
    "        available_in.append('Run 1')\n",
    "        a = next(a for a in runs_data['poxmea6n']['artifacts'] if a['name'] == artifact_name)\n",
    "        version1 = a['version']\n",
    "        artifact_type = a['type']\n",
    "    \n",
    "    if artifact_name in [a['name'] for a in runs_data['buqt4b6u']['artifacts']]:\n",
    "        available_in.append('Run 2')\n",
    "        a = next(a for a in runs_data['buqt4b6u']['artifacts'] if a['name'] == artifact_name)\n",
    "        version2 = a['version']\n",
    "        artifact_type = a['type']\n",
    "    \n",
    "    artifacts_data['Artifact'].append(artifact_name)\n",
    "    artifacts_data['Run 1 Version'].append(version1)\n",
    "    artifacts_data['Run 2 Version'].append(version2)\n",
    "    artifacts_data['Available In'].append(', '.join(available_in))\n",
    "    artifacts_data['Type'].append(artifact_type)\n",
    "\n",
    "df_artifacts = pd.DataFrame(artifacts_data)\n",
    "display(df_artifacts)\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Run 1 Files: {len(runs_data['poxmea6n']['files'])}\")\n",
    "print(f\"  Run 2 Files: {len(runs_data['buqt4b6u']['files'])}\")\n",
    "print(f\"  Run 1 Artifacts: {len(runs_data['poxmea6n']['artifacts'])}\")\n",
    "print(f\"  Run 2 Artifacts: {len(runs_data['buqt4b6u']['artifacts'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Images from Runs Side by Side\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def display_image_side_by_side(run1_image, run2_image, image_name):\n",
    "    \"\"\"Display two images side by side.\"\"\"\n",
    "    \n",
    "    def image_to_html(img, title):\n",
    "        \"\"\"Convert PIL image to HTML.\"\"\"\n",
    "        buffered = BytesIO()\n",
    "        img.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "        return f\"\"\"\n",
    "        <div style=\"text-align: center; flex: 1; min-width: 300px; padding: 10px;\">\n",
    "            <h4 style=\"color: {'#2e86de' if 'Run 1' in title else '#10ac84'}; margin-bottom: 10px;\">{title}</h4>\n",
    "            <img src=\"data:image/png;base64,{img_str}\" style=\"max-width: 100%; height: auto; border: 2px solid #ddd; border-radius: 5px;\">\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    if run1_image and run2_image:\n",
    "        html = f\"\"\"\n",
    "        <div style=\"display: flex; gap: 20px; flex-wrap: wrap; margin-bottom: 30px;\">\n",
    "            <div style=\"width: 100%; text-align: center; padding: 10px; background: #f8f9fa; border-radius: 5px; margin-bottom: 10px;\">\n",
    "                <h3 style=\"margin: 0;\">üñºÔ∏è {image_name}</h3>\n",
    "            </div>\n",
    "            {image_to_html(run1_image, 'Run 1 (poxmea6n)')}\n",
    "            {image_to_html(run2_image, 'Run 2 (buqt4b6u)')}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(html))\n",
    "    elif run1_image:\n",
    "        html = f\"\"\"\n",
    "        <div style=\"text-align: center; padding: 15px; background: #fff3cd; border-radius: 5px; margin-bottom: 30px;\">\n",
    "            <h3 style=\"color: #856404; margin: 0;\">üñºÔ∏è {image_name} - Only in Run 1</h3>\n",
    "        </div>\n",
    "        <div style=\"display: flex; justify-content: center;\">\n",
    "            {image_to_html(run1_image, 'Run 1 (poxmea6n)')}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(html))\n",
    "    elif run2_image:\n",
    "        html = f\"\"\"\n",
    "        <div style=\"text-align: center; padding: 15px; background: #fff3cd; border-radius: 5px; margin-bottom: 30px;\">\n",
    "            <h3 style=\"color: #856404; margin: 0;\">üñºÔ∏è {image_name} - Only in Run 2</h3>\n",
    "        </div>\n",
    "        <div style=\"display: flex; justify-content: center;\">\n",
    "            {image_to_html(run2_image, 'Run 2 (buqt4b6u)')}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(html))\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  No image found for: {image_name}\")\n",
    "\n",
    "def download_image(url):\n",
    "    \"\"\"Download image from URL and return PIL Image.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            return Image.open(BytesIO(response.content))\n",
    "    except Exception as e:\n",
    "        print(f\"  Error downloading image from {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def get_image_files(run_data):\n",
    "    \"\"\"Get all image files from run data.\"\"\"\n",
    "    image_extensions = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.webp'}\n",
    "    image_files = []\n",
    "    \n",
    "    for f in run_data['files']:\n",
    "        name_lower = f['name'].lower()\n",
    "        if any(name_lower.endswith(ext) for ext in image_extensions):\n",
    "            image_files.append(f)\n",
    "    \n",
    "    return image_files\n",
    "\n",
    "print(\"üñºÔ∏è Image Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get image files from both runs\n",
    "run1_images = get_image_files(runs_data['poxmea6n'])\n",
    "run2_images = get_image_files(runs_data['buqt4b6u'])\n",
    "\n",
    "print(f\"Run 1: {len(run1_images)} images found\")\n",
    "for img in run1_images:\n",
    "    size_str = f\"{img['size'] / 1024:.2f} KB\" if img['size'] else \"Unknown\"\n",
    "    print(f\"  - {img['name']} ({size_str})\")\n",
    "\n",
    "print(f\"\\nRun 2: {len(run2_images)} images found\")\n",
    "for img in run2_images:\n",
    "    size_str = f\"{img['size'] / 1024:.2f} KB\" if img['size'] else \"Unknown\"\n",
    "    print(f\"  - {img['name']} ({size_str})\")\n",
    "\n",
    "# Collect all unique image names for comparison\n",
    "all_image_names = set()\n",
    "all_image_names.update([img['name'] for img in run1_images])\n",
    "all_image_names.update([img['name'] for img in run2_images])\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Displaying Images Side by Side\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not all_image_names:\n",
    "    print(\"‚ö†Ô∏è  No images found in either run.\")\n",
    "else:\n",
    "    # Display each image comparison\n",
    "    for img_name in sorted(all_image_names):\n",
    "        # Get image from Run 1\n",
    "        run1_img_file = next((f for f in run1_images if f['name'] == img_name), None)\n",
    "        run1_image = None\n",
    "        if run1_img_file:\n",
    "            print(f\"Downloading from Run 1: {img_name}\")\n",
    "            run1_image = download_image(run1_img_file['url'])\n",
    "        \n",
    "        # Get image from Run 2\n",
    "        run2_img_file = next((f for f in run2_images if f['name'] == img_name), None)\n",
    "        run2_image = None\n",
    "        if run2_img_file:\n",
    "            print(f\"Downloading from Run 2: {img_name}\")\n",
    "            run2_image = download_image(run2_img_file['url'])\n",
    "        \n",
    "        # Display side by side\n",
    "        display_image_side_by_side(run1_image, run2_image, img_name)\n",
    "        print()\n",
    "\n",
    "print(\"‚úì Image comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Quick Links\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä RUN COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, run_id in enumerate(RUN_IDS, 1):\n",
    "    meta = runs_data[run_id]['meta']\n",
    "    summary = runs_data[run_id]['summary']\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ' * 80}\")\n",
    "    print(f\"üèÉ RUN {i}: {run_id}\")\n",
    "    print(f\"{'‚îÄ' * 80}\")\n",
    "    print(f\"Name:        {meta['name']}\")\n",
    "    print(f\"Display:     {meta['display_name']}\")\n",
    "    print(f\"State:       {meta['state']}\")\n",
    "    print(f\"Created:     {meta['created_at']}\")\n",
    "    print(f\"Tags:        {', '.join(meta['tags']) if meta['tags'] else 'None'}\")\n",
    "    print(f\"Group:       {meta['group'] or 'None'}\")\n",
    "    print(f\"Job Type:    {meta['job_type']}\")\n",
    "    print(f\"\\nüîó WandB URL: {meta['url']}\")\n",
    "    print(f\"\\nüìà History:   {len(runs_data[run_id]['history'])} steps, {len(runs_data[run_id]['history_keys'])} metrics\")\n",
    "    print(f\"üìÅ Files:     {len(runs_data[run_id]['files'])}\")\n",
    "    print(f\"üì¶ Artifacts: {len(runs_data[run_id]['artifacts'])}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Key Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nConfiguration parameters:\")\n",
    "print(f\"  Run 1: {len(runs_data['poxmea6n']['config'])} parameters\")\n",
    "print(f\"  Run 2: {len(runs_data['buqt4b6u']['config'])} parameters\")\n",
    "\n",
    "print(f\"\\nSummary metrics:\")\n",
    "print(f\"  Run 1: {len(runs_data['poxmea6n']['summary'])} metrics\")\n",
    "print(f\"  Run 2: {len(runs_data['buqt4b6u']['summary'])} metrics\")\n",
    "\n",
    "print(f\"\\nCommon summary metrics: {len(common_summary_keys)}\")\n",
    "print(f\"Common config keys: {len(common_config_keys)}\")\n",
    "print(f\"Common history metrics: {len(common_metrics)}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"‚úì Comparison complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notebook Guide\n",
    "\n",
    "This notebook provides a comprehensive side-by-side comparison of two WandB experiment runs.\n",
    "\n",
    "**What's included:**\n",
    "\n",
    "| Section | Description |\n",
    "|---------|-------------|\n",
    "| **Imports** | Loads required libraries for data manipulation and visualization |\n",
    "| **Run Definitions** | Defines the two run IDs: `poxmea6n` and `buqt4b6u` |\n",
    "| **Data Loading** | Fetches metadata, config, metrics, history, files, and artifacts from both runs |\n",
    "| **Comparison Tables** | Displays metadata, summary, and configuration side-by-side |\n",
    "| **Training History** | Plots time-series metrics for both runs with different colors |\n",
    "| **Performance Metrics** | Bar chart comparison of summary metrics |\n",
    "| **Difference Analysis** | Calculates and visualizes percentage differences between runs |\n",
    "| **Files/Artifacts** | Compares log files and artifacts between runs |\n",
    "| **Images** | Downloads and displays all images from both runs side by side for visual comparison |\n",
    "| **Summary** | Displays quick links and key statistics |\n",
    "\n",
    "**To compare different runs:**\n",
    "- Edit the `RUN_IDS` list in Cell 3\n",
    "\n",
    "**Color coding:**\n",
    "- üîµ **Blue**: Run 1 (`poxmea6n`)\n",
    "- üü¢ **Green**: Run 2 (`buqt4b6u`)\n",
    "- ‚úÖ **Green difference**: Run 2 performed better\n",
    "- ‚ùå **Red difference**: Run 1 performed better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
