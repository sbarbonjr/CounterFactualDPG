{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Compare Two WandB Runs Side by Side\n",
    "\n",
    "This notebook fetches and visualizes data from two experiment runs for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional, List\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Import wandb\n",
    "import wandb\n",
    "from wandb.apis.public import Run\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Experiment Tracking - Define Run IDs\n",
    "RUN_IDS = ['poxmea6n', 'buqt4b6u']\n",
    "ENTITY = 'mllab-ts-universit-di-trieste'\n",
    "PROJECT = 'CounterFactualDPG'\n",
    "\n",
    "print(f\"Comparing {len(RUN_IDS)} runs:\")\n",
    "for i, run_id in enumerate(RUN_IDS, 1):\n",
    "    print(f\"  {i}. {run_id}\")\n",
    "print(f\"\\nEntity: {ENTITY}\")\n",
    "print(f\"Project: {PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Run Data from WandB\n",
    "api = wandb.Api()\n",
    "\n",
    "def fetch_run_data(run_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Fetch comprehensive information from a WandB run.\"\"\"\n",
    "    run_path = f\"{ENTITY}/{PROJECT}/{run_id}\"\n",
    "    print(f\"Fetching run: {run_path}\")\n",
    "    \n",
    "    try:\n",
    "        run = api.run(run_path)\n",
    "        \n",
    "        # Collect all run information\n",
    "        run_data = {\n",
    "            'meta': {\n",
    "                'id': run.id,\n",
    "                'name': run.name,\n",
    "                'display_name': run.display_name,\n",
    "                'state': run.state,\n",
    "                'url': run.url,\n",
    "                'path': run.path,\n",
    "                'entity': run.entity,\n",
    "                'project': run.project,\n",
    "                'created_at': run.created_at,\n",
    "                'updated_at': getattr(run, 'updated_at', None),\n",
    "                'notes': run.notes,\n",
    "                'tags': list(run.tags) if run.tags else [],\n",
    "                'group': run.group,\n",
    "                'job_type': run.job_type,\n",
    "            },\n",
    "            'config': dict(run.config),\n",
    "            'summary': {},\n",
    "            'history': [],\n",
    "            'history_keys': [],\n",
    "            'system_metrics': {},\n",
    "            'files': [],\n",
    "            'artifacts': [],\n",
    "        }\n",
    "        \n",
    "        # Get summary metrics\n",
    "        for key, value in run.summary.items():\n",
    "            if not key.startswith('_'):\n",
    "                try:\n",
    "                    run_data['summary'][key] = float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    run_data['summary'][key] = value\n",
    "        \n",
    "        # Get history (time-series data)\n",
    "        try:\n",
    "            history = run.history(pandas=False)\n",
    "            if history:\n",
    "                run_data['history'] = list(history)\n",
    "                # Extract unique keys from history\n",
    "                all_keys = set()\n",
    "                for row in history:\n",
    "                    all_keys.update(row.keys())\n",
    "                run_data['history_keys'] = sorted(list(all_keys))\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not fetch history: {e}\")\n",
    "        \n",
    "        # Get files\n",
    "        try:\n",
    "            files = run.files()\n",
    "            run_data['files'] = [\n",
    "                {\n",
    "                    'name': f.name,\n",
    "                    'size': f.size,\n",
    "                    'mimetype': getattr(f, 'mimetype', None),\n",
    "                    'url': f.url,\n",
    "                }\n",
    "                for f in files\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not fetch files: {e}\")\n",
    "        \n",
    "        # Get artifacts\n",
    "        try:\n",
    "            artifacts = run.logged_artifacts()\n",
    "            run_data['artifacts'] = [\n",
    "                {\n",
    "                    'name': a.name,\n",
    "                    'type': a.type,\n",
    "                    'version': a.version,\n",
    "                    'size': a.size,\n",
    "                }\n",
    "                for a in artifacts\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not fetch artifacts: {e}\")\n",
    "        \n",
    "        print(f\"  ‚úì Successfully fetched {len(run_data['history'])} history steps\")\n",
    "        return run_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error fetching run: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch data for all runs\n",
    "runs_data = {}\n",
    "for run_id in RUN_IDS:\n",
    "    runs_data[run_id] = fetch_run_data(run_id)\n",
    "\n",
    "print(f\"\\n‚úì Loaded data for {len([r for r in runs_data.values() if r])}/{len(RUN_IDS)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Metrics and Parameters for Comparison\n",
    "def format_value(value, max_length=50):\n",
    "    \"\"\"Format values for display.\"\"\"\n",
    "    if value is None:\n",
    "        return \"N/A\"\n",
    "    if isinstance(value, (list, dict)):\n",
    "        return f\"{type(value).__name__}({len(value)})\"\n",
    "    str_val = str(value)\n",
    "    if len(str_val) > max_length:\n",
    "        return str_val[:max_length-3] + \"...\"\n",
    "    return str_val\n",
    "\n",
    "# Create comparison DataFrames\n",
    "comparison_data = {\n",
    "    'Metric': []\n",
    "}\n",
    "for i, run_id in enumerate(RUN_IDS):\n",
    "    comparison_data[f\"Run {i+1} ({run_id})\"] = []\n",
    "\n",
    "# Metadata comparison\n",
    "meta_fields = ['name', 'state', 'created_at', 'tags', 'group', 'job_type']\n",
    "for field in meta_fields:\n",
    "    comparison_data['Metric'].append(f\"meta.{field}\")\n",
    "    for i, run_id in enumerate(RUN_IDS):\n",
    "        comparison_data[f\"Run {i+1} ({run_id})\"].append(format_value(runs_data[run_id]['meta'].get(field)))\n",
    "\n",
    "# Summary metrics - find common keys\n",
    "all_summary_keys = [set(runs_data[run_id]['summary'].keys()) for run_id in RUN_IDS]\n",
    "common_summary_keys = sorted(set.intersection(*all_summary_keys))\n",
    "\n",
    "for key in common_summary_keys:\n",
    "    comparison_data['Metric'].append(f\"summary.{key}\")\n",
    "    for i, run_id in enumerate(RUN_IDS):\n",
    "        comparison_data[f\"Run {i+1} ({run_id})\"].append(format_value(runs_data[run_id]['summary'].get(key)))\n",
    "\n",
    "# Configuration comparison\n",
    "all_config_keys = [set(runs_data[run_id]['config'].keys()) for run_id in RUN_IDS]\n",
    "common_config_keys = sorted(set.intersection(*all_config_keys))\n",
    "\n",
    "for key in common_config_keys:\n",
    "    comparison_data['Metric'].append(f\"config.{key}\")\n",
    "    for i, run_id in enumerate(RUN_IDS):\n",
    "        comparison_data[f\"Run {i+1} ({run_id})\"].append(format_value(runs_data[run_id]['config'].get(key)))\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"‚úì Extracted metrics and parameters for comparison\")\n",
    "display(df_comparison.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Side-by-Side Comparison Tables\n",
    "\n",
    "# 1. Metadata Comparison\n",
    "meta_data = [runs_data[run_id]['meta'] for run_id in RUN_IDS]\n",
    "df_meta = pd.DataFrame(meta_data, index=[f\"Run {i+1} ({run_id})\" for i, run_id in enumerate(RUN_IDS)]).T\n",
    "\n",
    "# 2. Summary Metrics Comparison\n",
    "df_summary = pd.DataFrame({\n",
    "    f\"Run {i+1} ({run_id})\": runs_data[run_id]['summary']\n",
    "    for i, run_id in enumerate(RUN_IDS)\n",
    "})\n",
    "\n",
    "# 3. Configuration Comparison\n",
    "df_config = pd.DataFrame({\n",
    "    f\"Run {i+1} ({run_id})\": runs_data[run_id]['config']\n",
    "    for i, run_id in enumerate(RUN_IDS)\n",
    "})\n",
    "\n",
    "# Display side by side\n",
    "HTML(\"\"\"\n",
    "<div style=\"display: flex; gap: 20px; overflow-x: auto;\">\n",
    "    <div style=\"flex: 1;\">\n",
    "        <h3 style=\"color: #2e86de; margin-bottom: 10px;\">üìã Run Metadata</h3>\n",
    "        {meta_html}\n",
    "    </div>\n",
    "    <div style=\"flex: 1;\">\n",
    "        <h3 style=\"color: #10ac84; margin-bottom: 10px;\">üìä Summary Metrics</h3>\n",
    "        {summary_html}\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\".format(\n",
    "    meta_html=df_meta.to_html(classes=\"data-table\"),\n",
    "    summary_html=df_summary.head(20).to_html(classes=\"data-table\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training History - Side by Side\n",
    "\n",
    "# Find common history keys\n",
    "all_history_keys = [set(runs_data[run_id]['history_keys']) for run_id in RUN_IDS]\n",
    "common_metrics = sorted(set.intersection(*all_history_keys))\n",
    "\n",
    "print(f\"Common metrics found in history: {len(common_metrics)}\")\n",
    "print(f\"Metrics: {common_metrics}\")\n",
    "\n",
    "# Filter to numeric metrics\n",
    "numeric_metrics = []\n",
    "for key in common_metrics:\n",
    "    # Check if this is a numeric metric\n",
    "    is_numeric = True\n",
    "    for run_id in RUN_IDS:\n",
    "        for row in runs_data[run_id]['history']:\n",
    "            if key in row:\n",
    "                val = row[key]\n",
    "                if val is not None and not isinstance(val, (int, float)):\n",
    "                    is_numeric = False\n",
    "                    break\n",
    "    if is_numeric and any(key in row for run_id in RUN_IDS for row in runs_data[run_id]['history']):\n",
    "        numeric_metrics.append(key)\n",
    "\n",
    "print(f\"\\nNumeric metrics to plot: {numeric_metrics}\")\n",
    "\n",
    "# Create subplots for each metric\n",
    "if numeric_metrics:\n",
    "    n_metrics = min(len(numeric_metrics), 8)  # Limit to 8 metrics\n",
    "    n_cols = 2\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=n_rows, cols=n_cols,\n",
    "        subplot_titles=numeric_metrics[:n_metrics]\n",
    "    )\n",
    "    \n",
    "    colors = ['#2e86de', '#10ac84', '#9b59b6', '#f39c12', '#e74c3c', '#1abc9c', '#34495e'][:len(RUN_IDS)]\n",
    "    \n",
    "    for idx, metric in enumerate(numeric_metrics[:n_metrics]):\n",
    "        row = (idx // n_cols) + 1\n",
    "        col = (idx % n_cols) + 1\n",
    "        \n",
    "        for i, run_id in enumerate(RUN_IDS):\n",
    "            history = runs_data[run_id]['history']\n",
    "            steps = []\n",
    "            values = []\n",
    "            for row_data in history:\n",
    "                if metric in row_data:\n",
    "                    steps.append(row_data.get('_step', len(values)))\n",
    "                    values.append(row_data[metric])\n",
    "            \n",
    "            if steps and values:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=steps,\n",
    "                        y=values,\n",
    "                        mode='lines+markers',\n",
    "                        name=f\"Run {i+1} ({run_id})\",\n",
    "                        legendgroup=f\"group_{i}\" if idx == 0 else None,\n",
    "                        showlegend=(idx == 0),\n",
    "                        line=dict(color=colors[i], width=2),\n",
    "                        marker=dict(size=4)\n",
    "                    ),\n",
    "                    row=row, col=col\n",
    "                )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=300 * n_rows,\n",
    "        title_text=\"<b>Training History: Side-by-Side Comparison</b>\",\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No numeric metrics found in history to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Model Performance - Summary Metrics Bar Chart\n",
    "\n",
    "# Extract numeric summary metrics for comparison\n",
    "metric_values = []\n",
    "metric_names = []\n",
    "\n",
    "# Get all common summary keys\n",
    "all_summary_keys = [set(runs_data[run_id]['summary'].keys()) for run_id in RUN_IDS]\n",
    "common_summary_metrics = set.intersection(*all_summary_keys)\n",
    "\n",
    "for key in common_summary_metrics:\n",
    "    if not key.startswith('_') and isinstance(runs_data[RUN_IDS[0]]['summary'].get(key), (int, float)):\n",
    "        try:\n",
    "            values = []\n",
    "            for run_id in RUN_IDS:\n",
    "                values.append(float(runs_data[run_id]['summary'].get(key, 0)))\n",
    "            \n",
    "            # Filter out extreme values for better visualization\n",
    "            if all(abs(v) < 1e6 for v in values):\n",
    "                metric_names.append(key)\n",
    "                metric_values.append(values)\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "if metric_values:\n",
    "    column_names = [f\"Run {i+1} ({run_id})\" for i, run_id in enumerate(RUN_IDS)]\n",
    "    df_metrics = pd.DataFrame(metric_values, columns=column_names, index=metric_names)\n",
    "    \n",
    "    # Bar chart comparison\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    colors = ['#2e86de', '#10ac84', '#9b59b6', '#f39c12', '#e74c3c', '#1abc9c', '#34495e']\n",
    "    \n",
    "    for i, run_id in enumerate(RUN_IDS):\n",
    "        col_name = f\"Run {i+1} ({run_id})\"\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=col_name,\n",
    "            x=metric_names,\n",
    "            y=df_metrics[col_name],\n",
    "            marker_color=colors[i % len(colors)]\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='<b>Summary Metrics Comparison</b>',\n",
    "        xaxis_title='Metric',\n",
    "        yaxis_title='Value',\n",
    "        barmode='group',\n",
    "        height=600,\n",
    "        hovermode='x unified',\n",
    "        xaxis={'tickangle': -45},\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No numeric summary metrics found for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Difference Analysis\n",
    "\n",
    "# Calculate percentage differences (comparing first run to others)\n",
    "differences = []\n",
    "all_summary_keys = [set(runs_data[run_id]['summary'].keys()) for run_id in RUN_IDS]\n",
    "common_keys = set.intersection(*all_summary_keys)\n",
    "\n",
    "for key in common_keys:\n",
    "    if not key.startswith('_') and isinstance(runs_data[RUN_IDS[0]]['summary'].get(key), (int, float)):\n",
    "        try:\n",
    "            baseline_val = float(runs_data[RUN_IDS[0]]['summary'][key])\n",
    "            \n",
    "            # Compare with other runs\n",
    "            for i in range(1, len(RUN_IDS)):\n",
    "                run_id = RUN_IDS[i]\n",
    "                if (isinstance(runs_data[run_id]['summary'].get(key), (int, float))):\n",
    "                    compare_val = float(runs_data[run_id]['summary'][key])\n",
    "                    \n",
    "                    if abs(baseline_val) > 1e-10:  # Avoid division by zero\n",
    "                        pct_diff = ((compare_val - baseline_val) / abs(baseline_val)) * 100\n",
    "                        absolute_diff = compare_val - baseline_val\n",
    "                        \n",
    "                        # Filter for meaningful differences\n",
    "                        if abs(baseline_val) < 1e6 and abs(compare_val) < 1e6:\n",
    "                            differences.append({\n",
    "                                'Metric': key,\n",
    "                                'Baseline (Run 1)': baseline_val,\n",
    "                                'Comparison (Run ' + str(i+1) + ')': compare_val,\n",
    "                                'Absolute Diff': absolute_diff,\n",
    "                                'Percent Diff (%)': pct_diff,\n",
    "                                'Better Run': f'Run {i+1}' if pct_diff > 0 else 'Run 1',\n",
    "                                'Compared with': RUN_IDS[i]\n",
    "                            })\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "if differences:\n",
    "    df_diff = pd.DataFrame(differences).sort_values('Absolute Diff', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"Top Metric Differences:\")\n",
    "    df_diff_top = df_diff.head(15)\n",
    "    display(df_diff_top)\n",
    "    \n",
    "    # Visualization of differences\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Color based on whether Run 2 improved\n",
    "    colors = ['#10ac84' if diff > 0 else '#ee5253' for diff in df_diff_top['Percent Diff (%)']]\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_diff_top['Metric'],\n",
    "        y=df_diff_top['Percent Diff (%)'],\n",
    "        marker_color=colors,\n",
    "        hovertemplate='%{x}<br>Percent Change: %{y:.2f}%<extra></extra>',\n",
    "    ))\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", opacity=0.5)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='<b>Percentage Difference: Run 2 vs Run 1</b>',\n",
    "        xaxis_title='Metric',\n",
    "        yaxis_title='Percentage Difference (%)',\n",
    "        height=600,\n",
    "        xaxis={'tickangle': -45},\n",
    "        template='plotly_white',\n",
    "        annotations=[\n",
    "            dict(x=0.02, y=0.98, \n",
    "                 text=f'<span style=\"color:#10ac84\">‚óè Run {len(RUN_IDS)} better</span>',\n",
    "                 showarrow=False, xref='paper', yref='paper', \n",
    "                 xanchor='left', yanchor='top', bgcolor='rgba(255,255,255,0.9)',\n",
    "                 bordercolor='gray', borderwidth=1)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No comparable metrics found for difference analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Files and Artifacts\n",
    "\n",
    "print(\"üìÅ Files Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Build dynamic column names based on RUN_IDS\n",
    "files_data = {'File Name': []}\n",
    "for i, run_id in enumerate(RUN_IDS):\n",
    "    files_data[f'Run {i+1} Size (KB)'] = []\n",
    "files_data['Available In'] = []\n",
    "\n",
    "# Get all unique file names\n",
    "all_files = set()\n",
    "for run_id in RUN_IDS:\n",
    "    for f in runs_data[run_id]['files']:\n",
    "        all_files.add(f['name'])\n",
    "\n",
    "for filename in sorted(all_files):\n",
    "    available_in = []\n",
    "    for i, run_id in enumerate(RUN_IDS):\n",
    "        if filename in [f['name'] for f in runs_data[run_id]['files']]:\n",
    "            available_in.append(f'Run {i+1}')\n",
    "            f = next((f for f in runs_data[run_id]['files'] if f['name'] == filename), None)\n",
    "            if f:\n",
    "                files_data[f'Run {i+1} Size (KB)'].append(f\"{f['size'] / 1024:.2f}\" if f['size'] else \"Unknown\")\n",
    "        else:\n",
    "            files_data[f'Run {i+1} Size (KB)'].append(\"N/A\")\n",
    "    \n",
    "    files_data['File Name'].append(filename)\n",
    "    files_data['Available In'].append(', '.join(available_in))\n",
    "\n",
    "df_files = pd.DataFrame(files_data)\n",
    "display(df_files.head(20))\n",
    "\n",
    "print(\"\\nüì¶ Artifacts Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Build dynamic column names based on RUN_IDS\n",
    "artifacts_data = {'Artifact': [], 'Type': []}\n",
    "for i, run_id in enumerate(RUN_IDS):\n",
    "    artifacts_data[f'Run {i+1} Version'] = []\n",
    "artifacts_data['Available In'] = []\n",
    "\n",
    "# Get all unique artifact names\n",
    "all_artifacts = set()\n",
    "for run_id in RUN_IDS:\n",
    "    for a in runs_data[run_id]['artifacts']:\n",
    "        all_artifacts.add(a['name'])\n",
    "\n",
    "for artifact_name in sorted(all_artifacts):\n",
    "    available_in = []\n",
    "    artifact_type = \"N/A\"\n",
    "    \n",
    "    for i, run_id in enumerate(RUN_IDS):\n",
    "        if artifact_name in [a['name'] for a in runs_data[run_id]['artifacts']]:\n",
    "            available_in.append(f'Run {i+1}')\n",
    "            a = next((a for a in runs_data[run_id]['artifacts'] if a['name'] == artifact_name), None)\n",
    "            if a:\n",
    "                artifacts_data[f'Run {i+1} Version'].append(a['version'])\n",
    "                artifact_type = a['type']\n",
    "        else:\n",
    "            artifacts_data[f'Run {i+1} Version'].append(\"N/A\")\n",
    "    \n",
    "    artifacts_data['Artifact'].append(artifact_name)\n",
    "    artifacts_data['Type'].append(artifact_type)\n",
    "    artifacts_data['Available In'].append(', '.join(available_in))\n",
    "\n",
    "df_artifacts = pd.DataFrame(artifacts_data)\n",
    "display(df_artifacts)\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "for i, run_id in enumerate(RUN_IDS, 1):\n",
    "    print(f\"  Run {i} Files: {len(runs_data[run_id]['files'])}\")\n",
    "    print(f\"  Run {i} Artifacts: {len(runs_data[run_id]['artifacts'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Images from Runs Side by Side\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def display_images_side_by_side(images, image_name):\n",
    "    \"\"\"Display multiple images side by side.\"\"\"\n",
    "    \n",
    "    def image_to_html(img, title, color):\n",
    "        \"\"\"Convert PIL image to HTML.\"\"\"\n",
    "        if img is None:\n",
    "            return f\"\"\"\n",
    "            <div style=\"text-align: center; flex: 1; min-width: 300px; padding: 10px;\">\n",
    "                <h4 style=\"color: {color}; margin-bottom: 10px;\">{title}</h4>\n",
    "                <p style=\"color: #dc3545; font-style: italic;\">Image not found</p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        buffered = BytesIO()\n",
    "        img.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "        return f\"\"\"\n",
    "        <div style=\"text-align: center; flex: 1; min-width: 300px; padding: 10px;\">\n",
    "            <h4 style=\"color: {color}; margin-bottom: 10px;\">{title}</h4>\n",
    "            <img src=\"data:image/png;base64,{img_str}\" style=\"max-width: 100%; height: auto; border: 2px solid #ddd; border-radius: 5px;\">\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    colors = ['#2e86de', '#10ac84', '#9b59b6', '#f39c12', '#e74c3c', '#1abc9c', '#34495e']\n",
    "    \n",
    "    html_divs = []\n",
    "    for i, (run_id, image) in enumerate(zip(RUN_IDS, images)):\n",
    "        html_divs.append(image_to_html(image, f'Run {i+1} ({run_id})', colors[i % len(colors)]))\n",
    "    \n",
    "    image_divs = '\\n'.join(html_divs)\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <div style=\"display: flex; gap: 20px; flex-wrap: wrap; margin-bottom: 30px;\">\n",
    "        <div style=\"width: 100%; text-align: center; padding: 10px; background: #f8f9fa; border-radius: 5px; margin-bottom: 10px;\">\n",
    "            <h3 style=\"margin: 0;\">üñºÔ∏è {image_name}</h3>\n",
    "        </div>\n",
    "        {image_divs}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "def download_image(url):\n",
    "    \"\"\"Download image from URL and return PIL Image.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            return Image.open(BytesIO(response.content))\n",
    "    except Exception as e:\n",
    "        print(f\"  Error downloading image from {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def get_image_files(run_data):\n",
    "    \"\"\"Get all image files from run data.\"\"\"\n",
    "    image_extensions = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.webp'}\n",
    "    image_files = []\n",
    "    \n",
    "    for f in run_data['files']:\n",
    "        name_lower = f['name'].lower()\n",
    "        if any(name_lower.endswith(ext) for ext in image_extensions):\n",
    "            image_files.append(f)\n",
    "    \n",
    "    return image_files\n",
    "\n",
    "print(\"üñºÔ∏è Image Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get image files from all runs\n",
    "all_run_images = {}\n",
    "for run_id in RUN_IDS:\n",
    "    all_run_images[run_id] = get_image_files(runs_data[run_id])\n",
    "\n",
    "for i, run_id in enumerate(RUN_IDS):\n",
    "    print(f\"Run {i+1}: {len(all_run_images[run_id])} images found\")\n",
    "    for img in all_run_images[run_id]:\n",
    "        size_str = f\"{img['size'] / 1024:.2f} KB\" if img['size'] else \"Unknown\"\n",
    "        print(f\"  - {img['name']} ({size_str})\")\n",
    "\n",
    "# Collect all unique image names for comparison\n",
    "all_image_names = set()\n",
    "for run_id in RUN_IDS:\n",
    "    all_image_names.update([img['name'] for img in all_run_images[run_id]])\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Displaying Images Side by Side\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not all_image_names:\n",
    "    print(\"‚ö†Ô∏è  No images found in either run.\")\n",
    "else:\n",
    "    # Display each image comparison\n",
    "    for img_name in sorted(all_image_names):\n",
    "        # Download images from all runs\n",
    "        run_images = []\n",
    "        for run_id in RUN_IDS:\n",
    "            img_file = next((f for f in all_run_images[run_id] if f['name'] == img_name), None)\n",
    "            image = None\n",
    "            if img_file:\n",
    "                print(f\"Downloading from run {run_id}: {img_name}\")\n",
    "                image = download_image(img_file['url'])\n",
    "            run_images.append(image)\n",
    "        \n",
    "        # Display side by side\n",
    "        display(HTML(display_images_side_by_side(run_images, img_name)))\n",
    "        print()\n",
    "\n",
    "print(\"‚úì Image comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Quick Links\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä RUN COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, run_id in enumerate(RUN_IDS, 1):\n",
    "    meta = runs_data[run_id]['meta']\n",
    "    summary = runs_data[run_id]['summary']\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ' * 80}\")\n",
    "    print(f\"üèÉ RUN {i}: {run_id}\")\n",
    "    print(f\"{'‚îÄ' * 80}\")\n",
    "    print(f\"Name:        {meta['name']}\")\n",
    "    print(f\"Display:     {meta['display_name']}\")\n",
    "    print(f\"State:       {meta['state']}\")\n",
    "    print(f\"Created:     {meta['created_at']}\")\n",
    "    print(f\"Tags:        {', '.join(meta['tags']) if meta['tags'] else 'None'}\")\n",
    "    print(f\"Group:       {meta['group'] or 'None'}\")\n",
    "    print(f\"Job Type:    {meta['job_type']}\")\n",
    "    print(f\"\\nüîó WandB URL: {meta['url']}\")\n",
    "    print(f\"\\nüìà History:   {len(runs_data[run_id]['history'])} steps, {len(runs_data[run_id]['history_keys'])} metrics\")\n",
    "    print(f\"üìÅ Files:     {len(runs_data[run_id]['files'])}\")\n",
    "    print(f\"üì¶ Artifacts: {len(runs_data[run_id]['artifacts'])}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Key Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nConfiguration parameters:\")\n",
    "for i, run_id in enumerate(RUN_IDS, 1):\n",
    "    print(f\"  Run {i}: {len(runs_data[run_id]['config'])} parameters\")\n",
    "\n",
    "print(f\"\\nSummary metrics:\")\n",
    "for i, run_id in enumerate(RUN_IDS, 1):\n",
    "    print(f\"  Run {i}: {len(runs_data[run_id]['summary'])} metrics\")\n",
    "\n",
    "print(f\"\\nCommon summary metrics: {len(common_summary_keys)}\")\n",
    "print(f\"Common config keys: {len(common_config_keys)}\")\n",
    "print(f\"Common history metrics: {len(common_metrics)}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"‚úì Comparison complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notebook Guide\n",
    "\n",
    "This notebook provides a comprehensive side-by-side comparison of two WandB experiment runs.\n",
    "\n",
    "**What's included:**\n",
    "\n",
    "| Section | Description |\n",
    "|---------|-------------|\n",
    "| **Imports** | Loads required libraries for data manipulation and visualization |\n",
    "| **Run Definitions** | Defines run IDs in `RUN_IDS` constant (edit this to compare different runs) |\n",
    "| **Data Loading** | Fetches metadata, config, metrics, history, files, and artifacts from all runs |\n",
    "| **Comparison Tables** | Displays metadata, summary, and configuration side-by-side |\n",
    "| **Training History** | Plots time-series metrics for all runs with different colors |\n",
    "| **Performance Metrics** | Bar chart comparison of summary metrics |\n",
    "| **Difference Analysis** | Calculates and visualizes percentage differences between runs |\n",
    "| **Files/Artifacts** | Compares log files and artifacts between runs |\n",
    "| **Images** | Downloads and displays all images from all runs side by side for visual comparison |\n",
    "| **Summary** | Displays quick links and key statistics |\n",
    "\n",
    "**To compare different runs:**\n",
    "- Edit the `RUN_IDS` list in Cell 3\n",
    "\n",
    "**Color coding:**\n",
    "- üîµ **Blue**: Run 1 (`poxmea6n`)\n",
    "- üü¢ **Green**: Run 2 (`buqt4b6u`)\n",
    "- ‚úÖ **Green difference**: Run 2 performed better\n",
    "- ‚ùå **Red difference**: Run 1 performed better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
